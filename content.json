{"pages":[{"title":"404","text":"","path":"404/index.html","date":"04-24","excerpt":""},{"title":"about","text":"我是AiLo，曾就读于西安某高校。毕业后直接步入了程序员这一条路，目前是一个Android开发，做过音视频编解码、相机开发。业余时间博主也曾学习了解过深度学习，做过OCR图片识别项目。","path":"about/index.html","date":"04-24","excerpt":""},{"title":"categories","text":"","path":"categories/index.html","date":"04-25","excerpt":""},{"title":"search","text":"","path":"search/index.html","date":"04-24","excerpt":""},{"title":"tags","text":"","path":"tags/index.html","date":"04-25","excerpt":""}],"posts":[{"title":"Bitmap原始数据的存储与还原","text":"原创文章，未经作者允许不得转载 眉共春山争秀可怜长皱莫将清泪湿花枝，恐花也、如人瘦 概述Bitmap是Android图片处理这块绕不过的一个主题，在处理Bitmap缓存这方面，一般会分为两部分:内存缓存和磁盘缓存。磁盘缓存这块呢，常用的就是使用Bitmap的compress函数，根据实际需求压缩为想要的图片文件。对于常规的带有透明度的图片来说，选择无损压缩成PNG或者Webp文件，二次读取展示的Bitmap一般肉眼看不出差别。而对于一些特殊的Bitmap而言，这种存储方式就不再适用。譬如我最近遇到的一种情况，得到的Bitmap数据中，透明部分占了绝大多数。这种Bitmap可以毫无问题地直接展示出来。但存储为PNG或者Webp文件后，二次读取却缺损严重，根本无法还原。 磁盘缓存解决方案一将Bitmap毫无损耗的做磁盘缓存。我想的第一种方案，就是将Bitmap的所有数据存储为文件，对此可以利用他的copyPixelsToBuffer函数，对此扩展函数如下所示: 12345678fun Bitmap.saveUndamaged(dir:String)&#123; // 文件后缀可以是任意格式，只要是文件即可 val f = File(dir) val byteBuffer = ByteBuffer.allocate(byteCount) copyPixelsToBuffer(byteBuffer) val byteArray = byteBuffer.array() file.writeBytes(byteArray)&#125; 那么读取的时候，这个时候我们有了文件的ByteArray流，是不是只需要利用BitmapFactory.decodeByteArray这个函数就可以拿到原始位图了呢？答案是否定，存储的ByteArray磁盘文件只有原始位图的RGBA信息，缺失了位图的宽高，所以即使使用了BitmapFactory.decodeByteArray函数，也是无法还原位图的。此时，还原位图的真正方式如下： 123456// bitmap的宽高信息必须从外界传入fun getUndamagedBitmap(dir:String, size:Size):Bitmap&#123; val b = Bitmap.createBitmap(size.width, size.height, Bitmap.Config.ARGB_8888)b.copyPixelsFromBuffer(ByteBuffer.wrap(File(dir).readBytes())) return b&#125; 得到了位图的RGBA数据后，需要使用如上方式才能还原位图。首先我们创建一个等宽高的空白位图，然后将RGBA数据填充进去。这样就能远远本版的还原位图。但是务必注意这里的宽高一定要和原始位图相同，否则展示的位图将是错乱不堪 目前为止，这种方案实施起来还算可行，文件的读取速度还算可以接受。好一点的手机上，基本都在20ms左右徘徊【这个时间视位图数据量而定】。但这种方式同样有一个缺点，就是文件存储空间过大。相同的Bitmap，储存原始数据的文件比PNG图片要打上十几倍甚至几十倍。所以重点强调！！！如果手机磁盘空间吃紧的话，那么不建议使用这种方式。 解决方案二既然原始数据存储占用空间大，那么原始数据能不能再压缩呢？针对我遇到的这种情况，Bitmap大部分数据为0-纯透明，利用一些压缩算法来压缩，读取的时候再对数据还原是否可行呢？对此我进行了尝试，使用的是GZIP压缩，代码展示如下： 123456789101112fun Bitmap.saveUndamaged(dir: String) &#123; val byteBuffer = ByteBuffer.allocate(byteCount) copyPixelsToBuffer(byteBuffer) val byteArray = byteBuffer.array() // 这里的后缀同样可以是任意格式，存储不针对文件格式，只需要Byte数据 val fileOut = FileOutputStream(File(dir)) val zipOutputStream = GZIPOutputStream(fileOut) zipOutputStream.write(byteArray) zipOutputStream.close() fileOut.close()&#125; 那么，同理再读取时，需要对文件做解压缩处理，然后生成Bitmap： 123456val file = File(dir)val zip = GZIPInputStream(file)val bitmap = Bitmap.createBitmap(w, h, Bitmap.Config.ARGB_8888)bitmap.copyPixelsFromBuffer(ByteBuffer.wrap(zip.readBytes())) zip.close()file.close() 对数据做压缩处理后，空间占用会小很多。但相对图片文件来说，依然还是比较大的。同样的，用空间换时间，空间占用小了。读取时间必然就是长了，这种解压缩读取的话，时间相比上要比读取原始数据文件多了一两倍。孰轻孰重，还需根据需求自行定夺。 内存缓存同理。既然可以存储为文件，那么必然可以作内存缓存。只要稍微将上述方法，更换部分代码即可。缓存到内存中: 1234567891011121314class BitmapLru(val size: Size, val data: ByteArray)fun Bitmap.lruCache(): BitmapLru &#123; val array = byteArray() val out = ByteArrayOutputStream() val zip = GZIPOutputStream(out) zip.write(array) zip.close() // 在这里zip要及时关闭，否则读取压缩数据时会出现异常 val data = out.toByteArray() out.close() return BitmapLru(Size(width, height), data)&#125; 从内存中解压生成原始位图： 12345678910111213141516fun BitmapLru?.lruToBitmap(): Bitmap? &#123; this?.apply &#123; val s = System.currentTimeMillis() val inb = ByteArrayInputStream(data) val zip = GZIPInputStream(inb) val bitmap = Bitmap.createBitmap(size.width, size.height, Bitmap.Config.ARGB_8888) bitmap.copyPixelsFromBuffer(ByteBuffer.wrap(zip.readBytes())) zip.close() inb.close() Log.d(&quot;lruToBitmap&quot;, &quot;lru to bitmap cost :$&#123;System.currentTimeMillis() - s&#125; &quot;) return bitmap &#125; return null&#125; 在这里需要注意的是，在压缩数据时，一定要及时关闭GZIPOutputStream。否则在解压缩时，会抛出EOFException: Unexpected end of ZLIB input stream异常。 结语两种存储方案，就是空间和时间的选择问题。手机空间支持，就存储原始文件；空间吃紧，但是时间又允许，就选择压缩原始数据方案。好了~~~以上就是这次的分享，如果大家对音视频感兴趣的话，欢迎关注我的Github项目MediaLearn","path":"2019/06/20/2019-06-20-bitmap-compress/","date":"06-20","excerpt":"","tags":[{"name":"技术讨论","slug":"技术讨论","permalink":"https://yoursite.com/tags/技术讨论/"},{"name":"Android","slug":"Android","permalink":"https://yoursite.com/tags/Android/"}]},{"title":"Camera2录制视频(一)：音频的录制及编码","text":"原创文章，未经作者允许不得转载 山黛远，月波长暮云秋影蘸潇湘醉魂应逐凌波梦，分付西风此夜凉 在Android开发方面，音视频占据了不小领域。对于想往这方面了解的小伙伴们，往往不知道从何处下手开始学习。博主本人接触音视频开发有一段日子，作为自己学习的回顾和补充，也一直在记录一些音视频开发的博客。对往期博客有兴趣的朋友们可以先了解一二。 MediaCodeC硬编码将图片集编码为视频Mp4文件MediaCodeC编码视频MediaCodeC将视频完整解码，并存储为图片文件。使用两种不同的方式，硬编码解码视频MediaCodeC解码视频指定帧硬编码解码指定帧 概述最近再次回顾所学，觉得还有许多不足。遂决定写几篇小总结，以Android平台录制视频项目为例，整理自己的音视频开发知识。如果有小伙伴想学习音视频开发，但又不知道从何着手，可以模仿博主做一个相关的Demo来学习。本文的项目地址入口在Camera2Record入口界面，业务功能实现在Camera2Recorder。 API项目中视频方面采用的技术逻辑为： 使用Camera2API，配合MediaCodeC + Surface + OpenGL将原始帧数据编码为H264码流 音频方面采用技术逻辑为: AudioRecord录音，MediaCodeC将PCM数据编码为AAC数据 音视频编码使用的是MediaMuxer，将视频帧数据和音频帧数据封装为MP4文件。整体而言涉及到的API有： MediaCodeC AudioRecord MediaMuxer OpenGL（不用详细了解） 架构设计【注1】作为一个简单的音视频录制应用，并没有什么花哨的功能（暂时没有，以后会慢慢追加）。整体业务逻辑就是直截了当的录制视频 ——&gt; 产出视频。业务再细分的话，主要有三个部分：一是画面，即视频部分；二是声音，即音频部分；三是混合器，即将视频和音频混合，并生成视频文件。将业务略作区分后，我们由结果向前反推，既然要生成MP4文件，那么需要提供一些什么数据呢？所以我们根据输出——即混合器部分，梳理各个模块的详细功能。 视频封装 在混合器模块，使用了Android提供的MediaMuxer作为视频封装输出工具。MediaMuxer支持三种输出格式，分别为MP4、Webm和3GP文件，本次项目的混合器输出自然选择的是MP4文件。MP4是MPEG-4的官方容器格式定义的广义文件扩展名，可以流媒体化并支持众多多媒体的内容：多音轨、视频流、字幕、图片、可变帧率、码率【注2】。在制作MP4文件时，应该优先选用MPEG-4标准下的视频/音频格式，一般来说，对于MP4容器的封装，相对而言比较常见的有两种编码方式： H264视频编码，AAC音频编码 Xvid视频编码，MP4音频编码 视频编码算法在本项目中，博主采用的视频编码算法为H264。H264作为压缩率最高的视频压缩格式，与其他编码格式相比，同等画面质量，体积最小。它有两个名称，一个是沿用ITU_T组织的H.26x名称——H.264；另一个是MPEG-4AVC，AVC即为高级视频编码，而MP4格式则是H264编码制定使用的标准封装格式【注3】。 音频编码算法博主采用的音频编码算法为AAC。AAC可以同时支持48个音轨，15个低频音轨，相比MP3，AAC可以在体积缩小30%的前提下提供更好的音质【注4】。AAC最初是基于MPEG-2的音频编码技术，后来MPEG_4标准出台，AAC重新集成了其他技术，变更为现在的MPEG-4 AAC标准。一般而言，目前常用的AAC编码指代的就是MPEG-4 AAC。MPEG-4 AAC有六种子规格： MPEG-4 AAC LC 低复杂度规格（Low Complexity）—现在的手机比较常见的MP4文件中的音频部份就包括了该规格音频文件 MPEG-4 AAC Main 主规格 注：包含了除增益控制之外的全部功能，其音质最好 MPEG-4 AAC SSR 可变采样率规格（Scaleable SampleRate） MPEG-4 AAC LTP 长时期预测规格（Long TermPredicition） MPEG-4 AAC LD 低延迟规格（Low Delay） MPEG-4 AAC HE高效率规格（HighEfficiency）—这种规格用于低码率编码，有NeroACC 编码器支持 目前最流行的就是LC和HE了。需要注意的是MPEG-4 AAC LC这种规格为“低复杂度规格”，一般应用于中等码率。而中等码率，一般指96kbps~192kbps，所以如果使用了LC编码，请将码率控制在这个范围内会比较好一点。 工作流程将业务逻辑梳理清楚之后，那么各个模块更具体的功能就清晰了很多。这里有一个大致的工作流程图以作参考：先从视频模块开始，VideoRecorder运行在一个独立的工作线程，使用OpenGL+Surface+MediaCodeC对接Camera2，接受相机回调画面并编码为H264码流。这个类对外回调可用的视频帧数据VideoPacket对象。这个数据类型是工程中自行定义的对象，封装了这一帧视频的数据——ByteArray类型，以及这一帧数据携带的信息——BufferInfo:主要是这一帧的时间戳以及其他。接下来是音频模块，考虑到录音模块或许日后有机会复用，所以将录音模块单独分离出来。AudioRecorder在开始录制后不停运行，对外回调PCM原始数据——ByteArray类型。AudioRecord类可以对外提供两种类型，ShortArray和ByteArray，因为视频对外的数据类型为ByteArray，所以这里也选择了ByteArray。这一段PCM数据会被添加到一个外部的链表中，而AudioEncoder音频编码模块，也持有PCM数据链表。在开始录制后，AudioEncoder不断循环地从PCM链表中提取数据，编码为AAC格式的原始帧数据。这里的AAC原始数据，指的是没有添加ADTS头信息的数据。与此同时，视频模块输出的视频帧数据和音频模块输出的AAC音频帧数据，会被提交到Mux模块中，在这个模块中，持有两个视频帧数据和音频帧数据的链表。Mux模块会不断循环地从这两个链表中提取数据，使用MediaMuxer将帧数据封装到各自的轨上，最终输出MP4文件。 音频录制及编码音频模块分为录音以及编码两个小模块，分别运行在两个独立的工作线程。录音模块不用多提，完全是基于AudioRecord的二次封装，这里是代码地址AudioRecorder。这里主要说一下音频编码模块AudioEncoder，音频录制模块在运行后拿到可用PCM数据并回调到外部，封装到一个线程安全的链表中。而AudioEncoder则会不停地从链表中提取数据，再使用MediaCodeC将PCM数据编码为AAC格式的音频帧数据。由于MediaMuxer封装AAC音频轨，并不需要ADTS头信息，所以AudioEncoder得到的AAC原始帧数据也无须再作二次处理了。 1234567891011121314151617181920212223242526272829303132333435363738394041 var presentationTimeUs = 0L val bufferInfo = MediaCodec.BufferInfo() // 循环的拿取PCM数据，编码为AAC数据。 while (isRecording.isNotEmpty() || pcmDataQueue.isNotEmpty()) &#123; val bytes = pcmDataQueue.popSafe() bytes?.apply &#123; val (id, inputBuffer) = codec.dequeueValidInputBuffer(1000) inputBuffer?.let &#123; totalBytes += size it.clear() it.put(this) it.limit(size) // 当输入数据全部处理完，需要向Codec发送end——stream的Flag codec.queueInputBuffer(id, 0, size , presentationTimeUs, if (isEmpty()) MediaCodec.BUFFER_FLAG_END_OF_STREAM else 0) // 1000000L/ 总数据 / audio channel / sampleRate presentationTimeUs = 1000000L * (totalBytes / 2) / format.sampleRate &#125; &#125; loopOut@ while (true) &#123; // 获取可用的输出缓存队列 val outputBufferId = dequeueOutputBuffer(bufferInfo, defTimeOut) if (outputBufferId == MediaCodec.INFO_TRY_AGAIN_LATER) &#123; break@loopOut &#125; else if (outputBufferId == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) &#123; // audio format changed &#125; else if (outputBufferId &gt;= 0) &#123; if (bufferInfo.flags and MediaCodec.BUFFER_FLAG_END_OF_STREAM != 0) &#123; break@loopOut &#125; val outputBuffer = codec.getOutputBuffer(it) if (bufferInfo.size &gt; 0) &#123; frameCount++ dataCallback.invoke(outputBuffer, bufferInfo) &#125; codec.releaseOutputBuffer(it, false) &#125; &#125;&#125; 这里的工作流程是这样的：只有PCM链表中有数据，MediaCodeC就会将这些数据填入到可用的输入队列中。每一段PCM的数据长度并不一定是一帧音频数据所对应的长度，所以工程要做的是，不停地想编码器输入数据，而编码器也需要不停地往外输出数据，直至将编码器内部的输入数据编码完毕。还有一个需要注意的点，就是MediaCodec当输入数据全部填充完毕时，需要发送一个==BUFFER_FLAG_END_OF_STREAM==标示，用来标示数据输入END。如果没有发送这个标示的话，那么编码完后的音频数据会丢失掉最后一小段时间的音频。除此之外，还有一个很重要的点，就是AAC编码的时间戳计算问题，相关部分的知识请阅读博主之前的博客解决AAC编码时间戳问题 未完待续由于篇幅有限，这篇文章只分享了音频的编码，在下一篇文章里博主会分享视频的录制和编码~~以上 注 1、本文的架构设计参考了《音视频开发进阶指南》—— 实现一款视频录制应用章节 2、参考资料Mp4编码全介绍 3、参考资料音视频封装格式、编码格式知识 4、参考资料AAC音频编码格式介绍","path":"2019/06/01/2019-06-01-camera2-record1/","date":"06-01","excerpt":"","tags":[{"name":"Android","slug":"Android","permalink":"https://yoursite.com/tags/Android/"},{"name":"音视频","slug":"音视频","permalink":"https://yoursite.com/tags/音视频/"}]},{"title":"解决MediaMuxer编码AAC文件时间戳计算","text":"原创文章，转载请联系作者 西北望乡何处是，东南见月几回圆。昨风一吹无人会，今夜清光似往年。 主题音频是流式数据，并不像视频一样有P帧和B帧的概念。就像砌墙一样，咔咔往上摞就行了。一般来说，AAC编码中生成文件这一步，如果使用的是OutputStream流写入文件的话，就完全不需要计算时间。但在音视频同步或者使用Android自带的MediaMuxer来生成音频文件时，就需要计算音频帧的时间戳。 参考本文所涉及到的计算方法和API，为在Android环境下。使用AudioRecord音频录制，MediaCodeC编码AAC格式音频，同时使用MediaMuxer封装AAC格式音频文件。 方法AAC编码有两种计算时间戳的方式。第一种：使用PCM的数据量来计算；第二种：计算出AAC编码相应参数配置下，一帧的持续时间，再配合帧数来计算。 AAC编码、MediaMuxer生成文件伪代码MediaCodeC的AAC编码流程不再赘述，这里用伪代码来代替。主要是为了体现在代码何处设置时间戳： 12345678// MediaCodeC获得可用输入队列index = codeC.dequeueInputBuffer(......)// 当获取到可用输出队列时，我们将获取的PCM数据填入inputBuffer = codec.getInputBuffer(index)// 将PCM数据（ByteArray）填充到InputBufferinputBuffer.put(byteAarray——PCM数据)codec.queueInputBuffer(index, 0, byteArray的size , presentationTimeUs, 0) 在以上的伪代码中，presentationTimeUs就是需要我们设置时间戳的地方 填充PCM数据后，在得到MediaCodeC输出后，使用MedaMuxer写入数据，生成AAC文件。 123456789101112path = 输出路径。后缀aac、或者mp4mediaMuxer= MediaMuxer(path, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4)mediaMuxer.addTrack(音频轨)mediaMuxer.start()// codec拿到可用的输出数据。这些数据就是AAC格式的音频数据id = codec.dequeueOutputBuffer(bufferInfo, 10000)if(id &gt;= 0)&#123; outputBuffer = codec.getOutputBuffer(id) mediaMuxer.writeSamplet(audioTrack, outputBuffer, bufferInfo) &#125; 需要注意的是：使用MediaMuxer生成AAC音频文件时，不需要添加AAC头信息，直接写入即可。MediaMuxer写入文件时，BufferInfo这个参数就包含了这一帧数据的偏移、以及时间戳等信息。 更加完整的音频编码代码，请参考GitHub地址AudioEncoder 使用PCM的数据量来计算PCM是没有经过压缩的纯音频数据，我之前写过一篇音频入门的文章初识音频，记录了一些PCM相关的常识问题，感兴趣的可以去看看。PCM作为最原始的音频数据，可以根据大小来计算出时间，先给出公式： 1presentationTimeUs = 1000000L * (totalBytes / 2) / sampleRate 这是配置为采样率sampletRate、采样位数为16bit、单声道的PCM文件时间戳计算方式 接下来我们来分析以上公式的计算由来：假设有一段PCM文件，采样率为S,采样位数为n–(一般 采样位数的选择有4bit、8bit、16bit、32bit)，声道为单声道。那么在1s内，这段PCM的大小为： 1size = S * n * 1,单位为bit 众所周知，1 Byte = 8bit, 1 Short = 16bit。那么单位时间内，PCM的大小为: 12以byte为单位 = S * n * 1 / 8以short为单位 = S * n * 1 / 16 那么根据以上就可得到，配置参数为采样率sampleRate、16bit、声道为1的PCM文件，当传入编码器的总大小达到totalByte时，时间戳的计算方式： 12currents (微妙) = totalByte / (sampleRate * 16 * 1 / 8) = totalByte / 2 / sampleRate * 1000000L 当然如果选择以ShortArray来承载PCM数据的话，那么公式则变为: 12currents (微妙) = totalShort / (sampleRate * 16 * 1 / 16) = totalShort / sampletRate * 1000000L 使用AAC帧时间计算当编码器每输出一次数据，即可视作输出一帧AAC数据。一帧AAC原始数据包括1024个sample，那么AAC音频文件1s内的帧数为：sampleRate / 1024 帧。从而得到一帧AAC的持续时间为: 1perFrameTime (微妙) = 1000000L / sampleRate / 1024 已知每一帧的持续时间的话，那么只需要根据当前帧数，即可计算出当前的时间戳。 以上","path":"2019/05/20/2019-05-20-audio-time/","date":"05-20","excerpt":"","tags":[{"name":"Android","slug":"Android","permalink":"https://yoursite.com/tags/Android/"},{"name":"音视频","slug":"音视频","permalink":"https://yoursite.com/tags/音视频/"}]},{"title":"Camera2挖坑日记---如何解决预览画面变形","text":"原创文章，转载请联系作者 一梦觉来三十载，休休。空为梅花白了头 概述Camera2是目前Android相机开发最新的API，旧版本的Camera已经被废弃了。一般情况下，Camera2的使用是将图像发送到SurfaceView或者TextureView【通过SurfaceTexture】来预览。使用JPEG或者Raw sensor格式的ImageReader来捕获JPEG图像或RAW缓冲区。【注1】今天主要是记录一下， 在使用Camera2API开发Android相机过程中，解决预览画面变形的问题。另，本文所记录的情况，都是竖屏情况的设备。题主参考Google方法给出的Demo，自定义了TextureView，可以自动适配宽高，支持全屏展示。代码地址AutoFitTextureView 效果以下的画面基本上处于同一角度拍摄所得 变形的画面 正常的小画面，以宽为基准计算高度。 正常全屏画面，高度铺满屏幕，画面被拉近。 解决++在解决方案上，主要参考了Google官方给出的Demo。Camera2VideoFragment++。在Camera2的使用上，我使用了TextureView作为预览画面的承载。为什么不使用SurfaceView呢？因为SurfaceView是基于Window层面的View，有很多View的属性都用不了，使用起来比较麻烦。Camera2API会返回一系列可以用于输出到SurfaceTexture的Size集合。–++TextureView显示原理即是使用SurfaceTexture构建的Surface++ 如图：以题主手上的Oppo r15为例，总共会返回13个可以用作输出的size需要注意的是，如果以竖屏为例，这里的宽高是反过来的在得到可用的size集合后，根据实际开发情况选择合适的PreviewSize即可 正常小画面展示选定了一个合适的PreviewSize之后，只需要适配TextureView的宽高即可。小画面以宽为基准，需要根据屏幕宽度来计算相应的高度即可。这一部分的代码，在官方Demo里已经相当详细。其实很简单，就是自电影AutoFitTextureView里的onMeasure函数里，重新设定宽高。 12345678override fun onMeasure(....)&#123; if (width &lt; ((height * ratioWidth) / ratioHeight)) &#123; // 控件本身的宽小于根据比例计算来得宽，则使用控件本身的宽 setMeasuredDimension(width, (width * ratioHeight) / ratioWidth) &#125; else &#123; setMeasuredDimension((height * ratioWidth) / ratioHeight, height) &#125;&#125; 其中ratioWidth、ratioHeight即是PreviewSize 全屏展示全屏展示预览画面，则需要使用TextureView的另一个函数——setTransform。这个函数是给Textureview设置一个Transform，用于改变TextureView的画面。By,双指缩放时可以使用这个函数。全屏展示时，TextureView的宽高铺满整个屏幕，相应的我们只需要改变一下Transform即可，此时高度不变，但是要将画面的宽度放大。放大的倍数即为屏幕的高度除以小画面时计算得来的高度比例即可。还是在onMeasure函数内： 1234567891011override fun onmeasure(...)&#123; val w = resources.displayMetrics.widthPixels val h = resources.displayMetrics.heightPixels setMeasuredDimension(w, h) fullScreenTransform.reset() fullScreenTransform.set(defTransform) // 宽拉伸，高不变 fullScreenTransform.postScale(h.toFloat() /ratioHeight, 1f, w * 0.5f, h * 0.5f) setTransform(fullScreenTransform)&#125; 其中fullScreenTransform即为TextureView最初始的Transform 自定义TextureView题主将AutoFitTextureView重新封装了一下，对外提供了全屏展示的开关函数。地址在这里AutoFitTextureView，感兴趣的童鞋可以去看一下。以上","path":"2019/05/10/2019-05-10-camera2-1/","date":"05-10","excerpt":"","tags":[{"name":"Camera2","slug":"Camera2","permalink":"https://yoursite.com/tags/Camera2/"}],"preview":"https://raw.githubusercontent.com/JadynAi/oldpage.io/master/img/camera2_1_bg.jpg"},{"title":"使用MediaCodeC将图片集编码为视频","text":"原创文章，转载请联系作者 绿生莺啼春正浓，钗头青杏小，绿成丛。玉船风动酒鳞红。歌声咽，相见几时重？ 提要这是MediaCodeC系列的第三章，主题是如何使用MediaCodeC将图片集编码为视频文件。在Android多媒体的处理上，MediaCodeC是一套非常有用的API。此次实验中，所使用的图片集正是MediaCodeC硬解码视频，并将视频帧存储为图片文件文章中，对视频解码出来的图片文件集，总共332张图片帧。若是对MediaCodeC视频解码感兴趣的话，也可以浏览之前的文章：MediaCodeC解码视频指定帧，迅捷、精确 核心流程MediaCodeC的常规工作流程是：拿到可用输入队列，填充数据；拿到可用输出队列，取出数据，如此往复直至结束。在一般情况下，填充和取出两个动作并不是即时的，也就是说并不是压入一帧数据，就能拿出一帧数据。当然，除了编码的视频每一帧都是关键帧的情况下。一般情况下，输入和输出都使用buffer的代码写法如下： 12345678910for (;;) &#123; //拿到可用InputBuffer的id int inputBufferId = codec.dequeueInputBuffer(timeoutUs); if (inputBufferId &gt;= 0) &#123; ByteBuffer inputBuffer = codec.getInputBuffer(…); // inputBuffer 填充数据 codec.queueInputBuffer(inputBufferId, …); &#125; // 查询是否有可用的OutputBuffer int outputBufferId = codec.dequeueOutputBuffer(…); 本篇文章的编码核心流程，和以上代码相差不多。只是将输入Buffer替换成了Surface，使用Surface代替InputBuffer来实现数据的填充。 为什么使用Surface在MediaCodeC官方文档里有一段关于Data Type的描述： CodeC接受三种类型的数据，压缩数据（compressed data）、原始音频数据（raw audio data）以及原始视频数据（raw video data）。这三种数据都能被加工为ByteBuffer。但是对于原始视频数据，应该使用Surface去提升CodeC的性能。 在本次项目中，使用的是MediaCodeCcreateInputSurface函数创造出Surface，搭配OpenGL实现Surface数据输入。这里我画了一张简单的工作流程图：整体流程上其实和普通的MediaCodeC工作流程差不多，只不过是将输入源由Buffer换成了Surface。 知识点在代码中，MediaCodeC只负责数据的传输，而生成MP4文件主要靠的类是MediaMuxer。整体上，项目涉及到的主要API有： MediaCodeC，图片编码为帧数据 MediaMuxer，帧数据编码为Mp4文件 OpenGL，负责将图片绘制到Surface 接下来，我将会按照流程工作顺序，详解各个步骤： 流程详解在详解流程前，有一点要注意的是，工作流程中所有环节都必须处在同一线程。 配置首先，启动子线程。配置MediaCodeC： 12345678910111213var codec = MediaCodec.createEncoderByType(MediaFormat.MIMETYPE_VIDEO_AVC)// mediaFormat配置颜色格式、比特率、帧率、关键帧间隔// 颜色格式默认为MediaCodecInfo.CodecCapabilities.COLOR_FormatSurfacevar mediaFomat = MediaFormat.createVideoFormat(MediaFormat.MIMETYPE_VIDEO_AVC, size.width, size.height) .apply &#123; setInteger(MediaFormat.KEY_COLOR_FORMAT, colorFormat) setInteger(MediaFormat.KEY_BIT_RATE, bitRate) setInteger(MediaFormat.KEY_FRAME_RATE, frameRate) setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, iFrameInterval) &#125;codec.configure(mediaFormat, null, null, MediaCodec.CONFIGURE_FLAG_ENCODE)var inputSurface = codec.createInputSurface()codec.start() 将编码器配置好之后，接下来配置OpenGL的EGL环境以及GPU Program。由于OpenGL涉及到比较多的知识，在这里便不再赘述。视频编码项目中，为方便使用，我将OpenGL环境搭建以及GPU program搭建封装在了GLEncodeCore类中，感兴趣的可以看一下。EGL环境在初始化时，可以选择两种和设备连接的方式，一种是eglCreatePbufferSurface;另一种是eglCreateWindowSurface,创建一个可实际显示的windowSurface，需要传一个Surface参数，毫无疑问选择这个函数。 123456789var encodeCore = GLEncodeCore(...)encodeCore.buildEGLSurface(inputSurface)fun buildEGLSurface(surface: Surface) &#123; // 构建EGL环境 eglEnv.setUpEnv().buildWindowSurface(surface) // GPU program构建 encodeProgram.build()&#125; 图片数据传入，并开始编码在各种API配置好之后，开启一个循环，将File文件读取的Bitmap传入编码。 12345678910val videoEncoder = VideoEncoder(640, 480, 1800000, 24)videoEncoder.start(Environment.getExternalStorageDirectory().path + &quot;/encodeyazi640$&#123;videoEncoder.bitRate&#125;.mp4&quot;)val file = File(图片集文件夹地址)file.listFiles().forEachIndexed &#123; index, it -&gt; BitmapFactory.decodeFile(it.path)?.apply &#123; videoEncoder.drainFrame(this, index) &#125;&#125;videoEncoder.drainEnd() 在提要里面也提到了，编码项目使用的图片集是之前MediaCodeC硬解码视频，并将视频帧存储为图片文件中的视频文件解码出来的，332张图片。循环代码中，我们逐次将图片Bitmap传入drainFrame(...)函数，用于编码。当所有帧编码完成后，使用drainEnd函数通知编码器编码完成。 视频帧编码接着我们再来看drameFrame(...)函数中的具体实现。 12345678910111213141516171819/** * * @b : draw bitmap to texture * * @presentTime: frame current time * */ fun drainFrame(b: Bitmap, presentTime: Long) &#123; encodeCore.drainFrame(b, presentTime) drainCoder(false) &#125; fun drainFrame(b: Bitmap, index: Int) &#123; drainFrame(b, index * mediaFormat.perFrameTime * 1000) &#125; fun drainCoder(...)&#123; 伪代码：MediaCodeC拿到输出队列数据，使用MediaMuxer编码为 Mp4文件 &#125; 首先使用OpenGL将Bitmap绘制纹理上，将数据传输到Surface上，并且需要将这个Bitmap所代表的时间戳传入。在传入数据后使用drainCoder函数，从MediaCodeC读取输出数据，使用MediaMuxer编码为Mp4视频文件。drainCoder函数具体实现如下： 123456789101112131415161718loopOut@ while (true) &#123; // 获取可用的输出缓存队列 val outputBufferId = dequeueOutputBuffer(bufferInfo, defTimeOut) Log.d(&quot;handleOutputBuffer&quot;, &quot;output buffer id : $outputBufferId &quot;) if (outputBufferId == MediaCodec.INFO_TRY_AGAIN_LATER) &#123; if (needEnd) &#123; // 输出无响应 break@loopOut &#125; &#125; else if (outputBufferId == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) &#123; // 输出数据格式改变，在这里启动mediaMuxer &#125; else if (outputBufferId &gt;= 0) &#123; // 拿到相应的输出数据 if (bufferInfo.flags and MediaCodec.BUFFER_FLAG_END_OF_STREAM != 0) &#123; break@loopOut &#125; &#125; &#125; 就像之前提到过的，并不是压入一帧数据就能即时得到一帧数据。在使用OpenGL将Bitmap绘制到纹理上，并传到Surface之后。要想得到输出数据，必须在一个无限循环的代码中，去拿MediaCodeC输出数据。也就是在这里的代码中，当输出数据格式改变时，为MediaMuxer加上视频轨，并启动。 12trackIndex = mediaMuxer!!.addTrack(codec.outputFormat)mediaMuxer!!.start() 整体上的工作流程就是以上这些代码了，传入一帧数据到Surface–&gt;MediaCodeC循环拿输出数据–&gt; MediaMuxer写入Mp4视频文件。当然，后两步的概念已经相对比较清晰，只有第一步的实现是一个难点，也是当时比较困扰我的一点。接下来我们将会详解，如何将一个Bitmap通过OpenGL把数据传输到Surface上。 Bitmap –&gt; Surface项目中，将Bitmap数据传输到Surface上，主要靠这一段代码： 123456fun drainFrame(b: Bitmap, presentTime: Long) &#123; encodeProgram.renderBitmap(b) // 给渲染的这一帧设置一个时间戳 eglEnv.setPresentationTime(presentTime) eglEnv.swapBuffers()&#125; 其中encodeProgram是显卡绘制程序，它内部会生成一个纹理，然后将Bitmap绘制到纹理上。此时这个纹理就代表了这张图片，再将纹理绘制到窗口上。之后，使用EGL的swapBuffer提交当前渲染结果，在提交之前，使用setPresentationTime提交当前帧代表的时间戳。 更加具体的代码实现，都在我的Github项目中。GLEncodeCore以及EncodeProgram GPU Program还有EGL 环境构建 结语此处有项目地址，点击传送","path":"2019/04/01/2019-04-01-MediaCodeC-encoder1/","date":"04-01","excerpt":"","tags":[{"name":"Android","slug":"Android","permalink":"https://yoursite.com/tags/Android/"},{"name":"视频","slug":"视频","permalink":"https://yoursite.com/tags/视频/"},{"name":"MediaCodeC","slug":"MediaCodeC","permalink":"https://yoursite.com/tags/MediaCodeC/"},{"name":"OpenGL","slug":"OpenGL","permalink":"https://yoursite.com/tags/OpenGL/"}],"preview":"https://raw.githubusercontent.com/JadynAi/oldpage.io/master/img/mention.jpg"},{"title":"MediaCodeC解码视频指定帧，迅捷、精确","text":"原创文章，转载请联系作者 若待明朝风雨过，人在天涯！春在天涯 提要最近在整理硬编码MediaCodec相关的学习笔记，以及代码文档，分享出来以供参考。本人水平有限，项目难免有思虑不当之处，若有问题可以提Issues。项目地址传送门此篇文章，主要是分享如何用MediaCodeC解码视频指定时间的一帧，回调Bitmap对象。之前还有一篇MediaCodeC硬解码视频，并将视频帧存储为图片文件，主要内容是将视频完整解码，并存储为JPEG文件，大家感兴趣可以去看一看。 如何使用VideoDecoder2上手简单直接，首先需要创建一个解码器对象： 1val videoDecoder2 = VideoDecoder2(dataSource) dataSoure就是视频文件地址 解码器会在对象创建的时候，对视频文件进行分析，得出时长、帧率等信息。有了解码器对象后，在需要解码帧的地方，直接调用函数： 12345videoDecoder2.getFrame(time, &#123; it-&gt; //成功回调，it为对应帧Bitmap对象 &#125;, &#123; //失败回调 &#125;) time 接受一个Float数值，级别为秒 getFrame函数式一个异步回调，会自动回调到主线程里来。同时这个函数也没有过度调用限制。也就是说——，你可以频繁调用而不用担心出现其他问题。 代码结构、实现过程代码结构VideoDecoder2目前只支持硬编码解码，在某些机型或者版本下，可能会出现兼容问题。后续会继续补上软解码的功能模块。先来看一下VideoDecoder2的代码框架，有哪些类构成，以及这些类起到的作用。在VideoDecoder2中，DecodeFrame承担着核心任务，由它发起这一帧的解码工作。获取了目标帧的YUV数据后；由GLCore来将这一帧转为Bitmap对象，它内部封装了OpenGL环境的搭建，以及配置了Surface供给MediaCodeC使用。FrameCache主要是做着缓存的工作，内部有内存缓存LruCache以及磁盘缓存DiskLruCache，因为缓存的存在，很大程度上提高了二次读取的效率。 工作流程VideoDecoder2的工作流程，是一个线性任务队列串行的方式。其工作流程图如下：具体流程： 1.当执行getFrame函数时，首先从缓存从获取这一帧的图片缓存。 2.如果缓存中没有这一帧的缓存，那么首先判断任务队列中正在执行的任务是否和此时需要的任务重复，如果不重复，则创建一个DecodeFrame任务加入队列。 3.任务队列的任务是在一个特定的子线程内，线性执行。新的任务会被加入队列尾端，而已有任务则会被提高优先级，移到队列中index为1的位置。 4、DecodeFrame获取到这一帧的Bitmap后，会将这一帧缓存为内存缓存，并在会在缓存线程内作磁盘缓存，方便二次读取。 接下来分析一下，实现过程中的几个重要的点。 实现过程 如何定位和目标时间戳相近的采样点 如何使用MediaCodeC获取视频特定时间帧 缓存是如何工作，起到的作用有哪些 定位精确帧精确其实是一个相对而言的概念，MediaExtractor的seekTo函数，有三个可供选择的标记：SEEK_TO_PREVIOUS_SYNC, SEEK_TO_CLOSEST_SYNC, SEEK_TO_NEXT_SYNC，分别是seek指定帧的上一帧，最近帧和下一帧。其实，seekTo并无法每次都准确的跳到指定帧，这个函数只会seek到目标时间的最接近的（CLOSEST）、上一帧（PREVIOUS）和下一帧（NEXT）。因为视频编码的关系，解码器只会从关键帧开始解码，也就是I帧。因为只有I帧才包含完整的信息。而P帧和B帧包含的信息并不完全，只有依靠前后帧的信息才能解码。所以这里的解决办法是：先定位到目标时间的上一帧，然后advance，直到读取的时间和目标时间的差值最小，或者读取的时间和目标时间的差值小于帧间隔。 12345678910111213141516171819202122232425262728293031323334353637383940414243val MediaFormat.fps: Int get() = try &#123; getInteger(MediaFormat.KEY_FRAME_RATE) &#125; catch (e: Exception) &#123; 0 &#125;/* * * return : 每一帧持续时间，微秒 * */ val perFrameTime by lazy &#123; 1000000L / mediaFormat.fps &#125;/* * * 查找这个时间点对应的最接近的一帧。 * 这一帧的时间点如果和目标时间相差不到 一帧间隔 就算相近 * * maxRange:查找范围 * */ fun getValidSampleTime(time: Long, @IntRange(from = 2) maxRange: Int = 5): Long &#123; checkExtractor.seekTo(time, MediaExtractor.SEEK_TO_PREVIOUS_SYNC) var count = 0 var sampleTime = checkExtractor.sampleTime while (count &lt; maxRange) &#123; checkExtractor.advance() val s = checkExtractor.sampleTime if (s != -1L) &#123; count++ // 选取和目标时间差值最小的那个 sampleTime = time.minDifferenceValue(sampleTime, s) if (Math.abs(sampleTime - time) &lt;= perFrameTime) &#123; //如果这个差值在 一帧间隔 内，即为成功 return sampleTime &#125; &#125; else &#123; count = maxRange &#125; &#125; return sampleTime &#125; 帧间隔其实就是：1s/帧率 使用MediaCodeC解码指定帧获取到相对精确的采样点（帧）后，接下来就是使用MediaCodeC解码了。首先，使用MediaExtractor的seekTo函数定位到目标采样点。 1mediaExtractor.seekTo(time, MediaExtractor.SEEK_TO_PREVIOUS_SYNC) 然后MediaCodeC将MediaExtractor读取的数据压入输入队列，不断循环，直到拿到想要的目标帧的数据。 123456789101112131415161718192021222324252627282930313233343536373839404142/** 持续压入数据，直到拿到目标帧* */private fun handleFrame(time: Long, info: MediaCodec.BufferInfo, emitter: ObservableEmitter&lt;Bitmap&gt;? = null) &#123; var outputDone = false var inputDone = false videoAnalyze.mediaExtractor.seekTo(time, MediaExtractor.SEEK_TO_PREVIOUS_SYNC) while (!outputDone) &#123; if (!inputDone) &#123; decoder.dequeueValidInputBuffer(DEF_TIME_OUT) &#123; inputBufferId, inputBuffer -&gt; val sampleSize = videoAnalyze.mediaExtractor.readSampleData(inputBuffer, 0) if (sampleSize &lt; 0) &#123; decoder.queueInputBuffer(inputBufferId, 0, 0, 0L, MediaCodec.BUFFER_FLAG_END_OF_STREAM) inputDone = true &#125; else &#123; // 将数据压入到输入队列 val presentationTimeUs = videoAnalyze.mediaExtractor.sampleTime Log.d(TAG, &quot;$&#123;if (emitter != null) &quot;main time&quot; else &quot;fuck time&quot;&#125; dequeue time is $presentationTimeUs &quot;) decoder.queueInputBuffer(inputBufferId, 0, sampleSize, presentationTimeUs, 0) videoAnalyze.mediaExtractor.advance() &#125; &#125; decoder.disposeOutput(info, DEF_TIME_OUT, &#123; outputDone = true &#125;, &#123; id -&gt; Log.d(TAG, &quot;out time $&#123;info.presentationTimeUs&#125; &quot;) if (decodeCore.updateTexture(info, id, decoder)) &#123; if (info.presentationTimeUs == time) &#123; // 遇到目标时间帧，才生产Bitmap outputDone = true val bitmap = decodeCore.generateFrame() frameCache.cacheFrame(time, bitmap) emitter?.onNext(bitmap) &#125; &#125; &#125;) &#125; decoder.flush()&#125; 需要注意的是，解码的时候，并不是压入一帧数据，就能得到一帧输出数据的。常规的做法是，持续不断向输入队列填充帧数据，直到拿到想要的目标帧数据。原因还是因为视频帧的编码，并不是每一帧都是关键帧，有些帧的解码必须依靠前后帧的信息。 缓存 LruCache，内存缓存 DiskLruCache LruCache自不用多说，磁盘缓存使用的是著名的DiskLruCache。缓存在VideoDecoder2中占有很重要的位置，它有效的提高了解码器二次读取的效率，从而不用多次解码以及使用OpenGL绘制。 之前在Oppo R15的测试机型上，进行了一轮解码测试。使用MediaCodeC解码一帧到到的Bitmap，大概需要100~200ms的时间。而使用磁盘缓存的话，读取时间大概在50~60ms徘徊，效率增加了一倍。 在磁盘缓存使用的过程中，有对DiskLruCache进行二次封装，内部使用单线程队列形式。进行磁盘缓存，对外提供了异步和同步两种方式获取缓存。可以直接搭配DiskLruCache使用——DiskCacheAssist.kt 总结到目前为止，视频解码的部分已经完成。上一篇是对视频完整解码并存储为图片文件，MediaCodeC硬解码视频，并将视频帧存储为图片文件，这一篇是解码指定帧。音视频相关的知识体系还很大，会继续学习下去。 结语此处有项目地址，点击传送","path":"2019/02/09/2019-02-09-MediaCodeC-frame/","date":"02-09","excerpt":"","tags":[{"name":"Android","slug":"Android","permalink":"https://yoursite.com/tags/Android/"},{"name":"视频","slug":"视频","permalink":"https://yoursite.com/tags/视频/"},{"name":"MediaCodeC","slug":"MediaCodeC","permalink":"https://yoursite.com/tags/MediaCodeC/"}]},{"title":"MediaCodeC硬解码视频，并将视频帧存储为图片文件","text":"原创文章，转载请联系作者 醉拍春衫惜旧香，天将离恨恼疏狂。年年陌上生秋草，日日楼中到夕阳。 目的 MediaCodeC搭配MediaExtractor将视频完整解码 视频帧存储为JPEG文件 使用两种方式达成 硬编码输出数据二次封装为YuvImage，并直接输出为JPEG格式文件 硬编码搭配Surface，用OpenGL封装为RGBA数据格式，再利用Bitmap压缩为图片文件 二者皆可以调整图片输出质量 参考 YUV的处理方式，强推大家观看这篇文章高效率得到YUV格式帧，绝对整的明明白白 OpenGL的处理方式，当然是最出名的BigFlake，硬编码相关的示例代码很是详细 解码效率分析 参考对象为一段约为13.8s，H.264编码，FPS为24，72*1280的MPEG-4的视频文件。鸭鸭戏水视频 此视频的视频帧数为332 略好点的设备解码时间稍短一点。但两种解码方式的效率对比下来，OpenGl渲染耗费的时间比YUV转JPEG多。 另：差一点的设备上，这个差值会被提高，约为一倍多。较好的设备，则小于一倍。 实现过程对整个视频的解析，以及压入MediaCodeC输入队列都是通用步骤。 123456789101112131415161718192021mediaExtractor.setDataSource(dataSource)// 查看是否含有视频轨val trackIndex = mediaExtractor.selectVideoTrack()if (trackIndex &lt; 0) &#123; throw RuntimeException(&quot;this data source not video&quot;)&#125;mediaExtractor.selectTrack(trackIndex) fun MediaExtractor.selectVideoTrack(): Int &#123; val numTracks = trackCount for (i in 0 until numTracks) &#123; val format = getTrackFormat(i) val mime = format.getString(MediaFormat.KEY_MIME) if (mime.startsWith(&quot;video/&quot;)) &#123; return i &#125; &#125; return -1&#125; 配置MediaCodeC解码器，将解码输出格式设置为COLOR_FormatYUV420Flexible，这种模式几乎所有设备都会支持。使用OpenGL渲染的话，MediaCodeC要配置一个输出Surface。使用YUV方式的话，则不需要配置 12345678910111213141516171819202122232425262728293031323334353637383940414243outputSurface = if (isSurface) OutputSurface(mediaFormat.width, mediaFormat.height) else null // 指定帧格式COLOR_FormatYUV420Flexible,几乎所有的解码器都支持 if (decoder.codecInfo.getCapabilitiesForType(mediaFormat.mime).isSupportColorFormat(defDecoderColorFormat)) &#123; mediaFormat.setInteger(MediaFormat.KEY_COLOR_FORMAT, defDecoderColorFormat) decoder.configure(mediaFormat, outputSurface?.surface, null, 0) &#125; else &#123; throw RuntimeException(&quot;this mobile not support YUV 420 Color Format&quot;) &#125; val startTime = System.currentTimeMillis() Log.d(TAG, &quot;start decode frames&quot;) isStart = true val bufferInfo = MediaCodec.BufferInfo() // 是否输入完毕 var inputEnd = false // 是否输出完毕 var outputEnd = false decoder.start() var outputFrameCount = 0 while (!outputEnd &amp;&amp; isStart) &#123; if (!inputEnd) &#123; val inputBufferId = decoder.dequeueInputBuffer(DEF_TIME_OUT) if (inputBufferId &gt;= 0) &#123; // 获得一个可写的输入缓存对象 val inputBuffer = decoder.getInputBuffer(inputBufferId) // 使用MediaExtractor读取数据 val sampleSize = videoAnalyze.mediaExtractor.readSampleData(inputBuffer, 0) if (sampleSize &lt; 0) &#123; // 2019/2/8-19:15 没有数据 decoder.queueInputBuffer(inputBufferId, 0, 0, 0L, MediaCodec.BUFFER_FLAG_END_OF_STREAM) inputEnd = true &#125; else &#123; // 将数据压入到输入队列 val presentationTimeUs = videoAnalyze.mediaExtractor.sampleTime decoder.queueInputBuffer(inputBufferId, 0, sampleSize, presentationTimeUs, 0) videoAnalyze.mediaExtractor.advance() &#125; &#125; &#125; 可以大致画一个流程图如下： YUV通过以上通用的步骤后，接下来就是对MediaCodeC的输出数据作YUV处理了。步骤如下： 1.使用MediaCodeC的getOutputImage (int index)函数，得到一个只读的Image对象，其包含原始视频帧信息。 By：当MediaCodeC配置了输出Surface时，此值返回null 2.将Image得到的数据封装到YuvImage中，再使用YuvImage的compressToJpeg方法压缩为JPEG文件 YuvImage的封装，官方文档有这样一段描述：Currently only ImageFormat.NV21 and ImageFormat.YUY2 are supported。YuvImage只支持NV21或者YUY2格式，所以还可能需要对Image的原始数据作进一步处理，将其转换为NV21的Byte数组 读取Image信息并封装为Byte数组此次演示的机型，反馈的Image格式如下： getFormat = 35getCropRect().width()=720getCropRect().height()=1280 35代表ImageFormat.YUV_420_888格式。Image的getPlanes会返回一个数组，其中0代表Y，1代表U，2代表V。由于是420格式，那么四个Y值共享一对UV分量，比例为4：1。代码如下，参考YUV_420_888编码Image转换为I420和NV21格式byte数组,不过我这里只保留了NV21格式的转换 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475fun Image.getDataByte(): ByteArray &#123; val format = format if (!isSupportFormat()) &#123; throw RuntimeException(&quot;image can not support format is $format&quot;) &#125; // 指定了图片的有效区域，只有这个Rect内的像素才是有效的 val rect = cropRect val width = rect.width() val height = rect.height() val planes = planes val data = ByteArray(width * height * ImageFormat.getBitsPerPixel(format) / 8) val rowData = ByteArray(planes[0].rowStride) var channelOffset = 0 var outputStride = 1 for (i in 0 until planes.size) &#123; when (i) &#123; 0 -&gt; &#123; channelOffset = 0 outputStride = 1 &#125; 1 -&gt; &#123; channelOffset = width * height + 1 outputStride = 2 &#125; 2 -&gt; &#123; channelOffset = width * height outputStride = 2 &#125; &#125; // 此时得到的ByteBuffer的position指向末端 val buffer = planes[i].buffer // 行跨距 val rowStride = planes[i].rowStride // 行内颜色值间隔，真实间隔值为此值减一 val pixelStride = planes[i].pixelStride val TAG = &quot;getDataByte&quot; Log.d(TAG, &quot;planes index is $i&quot;) Log.d(TAG, &quot;pixelStride $pixelStride&quot;) Log.d(TAG, &quot;rowStride $rowStride&quot;) Log.d(TAG, &quot;width $width&quot;) Log.d(TAG, &quot;height $height&quot;) Log.d(TAG, &quot;buffer size &quot; + buffer.remaining()) val shift = if (i == 0) 0 else 1 val w = width.shr(shift) val h = height.shr(shift) buffer.position(rowStride * (rect.top.shr(shift)) + pixelStride + (rect.left.shr(shift))) for (row in 0 until h) &#123; var length: Int if (pixelStride == 1 &amp;&amp; outputStride == 1) &#123; length = w // 2019/2/11-23:05 buffer有时候遗留的长度，小于length就会报错 buffer.getNoException(data, channelOffset, length) channelOffset += length &#125; else &#123; length = (w - 1) * pixelStride + 1 buffer.getNoException(rowData, 0, length) for (col in 0 until w) &#123; data[channelOffset] = rowData[col * pixelStride] channelOffset += outputStride &#125; &#125; if (row &lt; h - 1) &#123; buffer.position(buffer.position() + rowStride - length) &#125; &#125; &#125; return data&#125; 最后封装YuvImage并压缩为文件1234val rect = image.cropRect val yuvImage = YuvImage(image.getDataByte(), ImageFormat.NV21, rect.width(), rect.height(), null) yuvImage.compressToJpeg(rect, 100, fileOutputStream) fileOutputStream.close() MediaCodeC配置输出Surface，使用OpenGL渲染OpenGL的环境搭建和渲染代码不再赘述，只是强调几个点： 渲染纹理的线程一定要和MediaCodeC配置Surface的线程保持一致 在渲染纹理代码前，一定要调用MediaCodeC的releaseOutputBuffer函数，将输出数据及时渲染到输出Surface上，否则Surface内的纹理将不会收到任何数据 获得可用的RGBA数据，使用Bitmap压缩为指定格式文件123456789101112131415fun saveFrame(fileName: String) &#123; pixelBuf.rewind() GLES20.glReadPixels(0, 0, width, height, GLES20.GL_RGBA, GLES20.GL_UNSIGNED_BYTE, pixelBuf) var bos: BufferedOutputStream? = null try &#123; bos = BufferedOutputStream(FileOutputStream(fileName)) val bmp = Bitmap.createBitmap(width, height, Bitmap.Config.ARGB_8888) pixelBuf.rewind() bmp.copyPixelsFromBuffer(pixelBuf) bmp.compress(Bitmap.CompressFormat.JPEG, 100, bos) bmp.recycle() &#125; finally &#123; bos?.close() &#125; &#125; 结果分析到目前为止，针对样例视频，YUV解码出来的视频帧亮度会稍低一点，且图片边缘处有细微的失真。OpenGL渲染解码的视频帧会明亮一些，放大三四倍边缘无失真。后续会继续追踪这个问题，会使用FFmpeg解码来作为对比。 结语此处有项目地址，点击传送","path":"2019/01/25/2019-01-25-MediaCodeC-Decode-1/","date":"01-25","excerpt":"","tags":[{"name":"Android","slug":"Android","permalink":"https://yoursite.com/tags/Android/"},{"name":"视频","slug":"视频","permalink":"https://yoursite.com/tags/视频/"},{"name":"MediaCodeC","slug":"MediaCodeC","permalink":"https://yoursite.com/tags/MediaCodeC/"}]},{"title":"使用A*算法求解机器人迷宫最短路径","text":"原创文章，转载请联系作者 时光只解催人老，不信多情，长恨离亭，泪滴春衫酒易醒。 前言最近接触了一个挺有意思的小课题，跟大家分享一下。就是利用A*算法，来计算迷宫可行路径。有关这个算法的知识，大家可以看看A星算法维基百科以及A星算法详解来稍作了解。代码地址在此Maze,喜欢Python的小可爱们可以拿去练练手。 提要说明本题中的迷宫，是以宫格类型呈现的，在代码中的呈现为二维数组。其次在迷宫中的移动，也只有上、下、左、右四个动作可选。如下所示： 其中1代表入口，2代表障碍物不可通行，3代表出口 12345[[3, 2, 2, 2, 2, 2, 2, 2, 1], [0, 0, 2, 2, 2, 2, 2, 0, 0], [2, 0, 0, 2, 2, 2, 0, 0, 2], [2, 2, 0, 0, 2, 0, 0, 2, 2], [2, 2, 2, 0, 0, 0, 2, 2, 2]] 其实在A*算法中，对单位搜索区域的描述为–节点nodes。在本题中，我们可以把搜索区域视为正方形，会更简单一点。 A*算法逻辑解析A*算法的逻辑其实并不是很难，简化起来就是两个词：评估、循环。从起点开始行动，首先找到起点周围可以行走的节点，然后在这个节点中，评估出距离终点最优（最短）的节点。那么这个最优节点，将作为下一步行动的点，以此类推，直至找到终点。可以看到，在这个逻辑中，其实最重要的就是评估这一步了。A*算法的评估函数为：f(n) = g(n) + h(n) g(n)–代表移动到这个点的代价，在本题中均为1.因为只可以水平或者数值运动。要是斜角可以移动的话，那么这个值就为√2h(n)–从这个点移动到终点的代价，这是一个猜测值。本题中，将迷宫视作坐标系的话，那么h(n)就是取和终点x、y各自差值的最小者。譬如点[4,2]和终点[1,1]的h(n)取值为：1 代码实现代码中对点的描述，均为实际值，并非以0为开始值计算。 定位起点和终点，使用列表存储四个移动命令，以下代码env_data代表迷宫数组：1234567891011121314# 上下左右四个移动命令，只具备四个移动命令orders = [&apos;u&apos;, &apos;d&apos;, &apos;l&apos;, &apos;r&apos;]# 定位起点和终点start_loc = []des_loc = []for index, value in enumerate(env_data, 1): if len(start_loc) == 0 or len(des_loc) != 0: if 1 in value: start_loc = (index, value.index(1) + 1) if 3 in value: des_loc = (index, value.index(3) + 1) else: break 判断节点所有可执行的移动命令：12345678910111213141516171819202122232425262728def valid_actions(loc): &quot;&quot;&quot; :param loc: :return: 当前位置所有可用的命令 &quot;&quot;&quot; loc_actions = [] for order in orders: if is_move_valid(loc, order): loc_actions.append(order) return loc_actionsdef is_move_valid(loc, act): &quot;&quot;&quot; 判断当前点，是否可使用此移动命令 &quot;&quot;&quot; x = loc[0] - 1 y = loc[1] - 1 if act not in orders: return false else: if act == orders[0]: return x != 0 and env_data[x - 1][y] != 2 elif act == orders[1]: return x != len(env_data) - 1 and env_data[x + 1][y] != 2 elif act == orders[2]: return y != 0 and env_data[x][y - 1] != 2 else: return y != len(env_data[0]) - 1 and env_data[x][y + 1] != 2 拿到节点周围移动单位为1的所有可到达点,不包括此节点：1234567891011121314151617181920212223242526272829303132def get_all_valid_loc(loc): &quot;&quot;&quot; 计算当前点，附近所有可用的点 :param loc: :return: &quot;&quot;&quot; all_valid_data = [] cur_acts = valid_actions(loc) for act in cur_acts: all_valid_data.append(move_robot(loc, act)) if loc in all_valid_data: all_valid_data.remove(loc) return all_valid_data def move_robot(loc, act): &quot;&quot;&quot; 移动机器人，返回新位置 :param loc: :param act: :return: &quot;&quot;&quot; if is_move_valid(loc, act): if act == orders[0]: return loc[0] - 1, loc[1] elif act == orders[1]: return loc[0] + 1, loc[1] elif act == orders[2]: return loc[0], loc[1] - 1 else: return loc[0], loc[1] + 1 else: return loc h(n)函数体现：1234567def compute_cost(loc): &quot;&quot;&quot; 计算loc到终点消耗的代价 :param loc: :return: &quot;&quot;&quot; return min(abs(loc[0] - des_loc[0]), abs(loc[1] - des_loc[1])) 开始计算使用road_list来保存走过的路径，同时用另一个集合保存失败的节点——即此节点附近无可用节点，死胡同。 1234567891011121314151617181920212223242526# 已经走过的路径list，走过的路road_list = [start_loc]# 证实是失败的路径failed_list = []# 没有到达终点就一直循环while road_list[len(road_list) - 1] != des_loc: # 当前点 cur_loc = road_list[len(road_list) - 1] # 当前点四周所有可用点 valid_loc_data = get_all_valid_loc(cur_loc) # 如果可用点里包括已经走过的节点，则移除 for cl in road_list: if cl in valid_loc_data: valid_loc_data.remove(cl) # 如果可用点集合包括失败的节点，则移除 for fl in failed_list: if fl in valid_loc_data: valid_loc_data.remove(fl) # 没有可用点，视作失败，放弃该节点。从走过的路集合中移除掉 if len(valid_loc_data) == 0: failed_list.append(road_list.pop()) continue # 用评估函数对可用点集合排序，取末端的值，加入走过的路集合中 valid_loc_data.sort(key=compute_cost, reverse=True) road_list.append(valid_loc_data.pop()) 看运行结果 结语人生苦短，我用Python。代码地址在此Maze,喜欢Python的小可爱们可以拿去练练手。在研究迷宫的过程中，发现生成迷宫的算法也是很有意思的，等忙完这段时间再去研究研究。嘻~以上","path":"2018/07/29/2018-07-29-AStarmaze/","date":"07-29","excerpt":"","tags":[{"name":"算法","slug":"算法","permalink":"https://yoursite.com/tags/算法/"}]},{"title":"使用DSL模式构建Recyclerview适配器","text":"原创文章，转载请联系作者 前言这是Kotlin实践日记的第一章，使用Kotlin构建一个，使用方便、多功能的Recyclerview适配器——AcrobatAdapter。 AcrobatAdapter让开发者专注于Item的配置，包括Item的UI和数据显示，以及单击、双击、长按事件【且不会影响子View的事件传递】。而且不仅仅是单Item Style列表,还是多Item Style列表，AcrobatAdapter的使用都是一样方便简单。 AcrobatAdapter的设计灵感来自于夜叉，我的函数名称也沿用夜叉的函数名，因为itemDSL这个名称是在太贴切了。夜叉这个项目是针对Recyclerview整体来构建，推荐小可爱们去看看。 下面将展示使用文档。 使用文档####普通列表 123456789val acrobatAdapter = AcrobatAdapter&lt;Int&gt; &#123; itemDSL &#123; resId(R.layout.item_test) showItem &#123; d, pos, view -&gt; view.item_tv.text = &quot;数据Item&quot; + d &#125; &#125; &#125;.setData(数据源)recycler_view.adapter = acrobatAdapter 哒哒，以上代码就成功构建了一个列表resId()函数绑定Item的布局IDshowItem函数内，渲染数据，d表示此项Item对应数据，会根据适配器泛型自动转换。pos为Item的position，view是Item的布局View。而且Kotlin支持使用Id代表控件，再也不用写findViewById啦！最后，适配器的setData方法内置了DiffUtils，会对比新旧数据，这样在数据刷新时Recyclerview会有默认动画展示。让交互更加平滑。 如果开发者在项目中，有好几个界面的Item完全一样，那岂不是还要写几套代码？完全不用担心，Item也支持复用。首先Item相关类，继承AcrobatItem。 123456class Test : AcrobatItem&lt;Int&gt;() &#123; override fun showItem(d: Int, pos: Int, view: View) &#123; view.item_tv.text = &quot;共用Item&quot; + d &#125; override fun getResId(): Int = R.layout.item_test&#125; 在适配器中： 123456val acrobatAdapter = AcrobatAdapter&lt;Int&gt; &#123; item &#123; Test() &#125; &#125;.setData(数据源) recycler_view.adapter = acrobatAdapter 让你的Item继承AcrobatItem即可。这样再多的界面复用Item也完全可行，但要注意的一点是：Item的数据类型必须一致。 多Item样式可以咩？当然可以啦！多item样式的话，写多个ItemDSL即可！每一个ItemDSL就代表一种独有的Item样式。同理，每调用一次item，也就多一种Item样式。 1234567891011121314151617val acrobatAdapter = AcrobatAdapter&lt;Int&gt; &#123; itemDSL &#123; resId(R.layout.item_test) showItem &#123; d, pos, view -&gt; view.item_tv.text = &quot;数据Item: &quot; + d &#125; isMeetData &#123; d, pos -&gt; pos == 1 &#125; &#125; itemDSL &#123; resId(R.layout.item_test1) showItem &#123; d, pos, view -&gt; view.item_tv.text = &quot;cece: &quot; + d &#125; isMeetData &#123; d, pos -&gt; pos != 1 &#125; &#125; &#125;.setData(数据源) recycler_view.adapter = acrobatAdapter isMeetData函数两个参数为数据和position。用这两个参数来判断此Item在哪个位置、什么条件展示。譬如示例代码，position为1时是一个样式，不为1时是另一种样式。但要注意，所有的Item的isMeetData的条件都是互斥的噢。否则会抛出异常。 嗯……Recyclerview没有默认的Item点击事件怎么办？没问题，AcrobatAdapter替你搞定。每个Item不但有单击click,还附带了双击DoubleTap和长按LongPress事件。而且完全不会影响Item布局View内部childView的事件。 每种Item都可以绑定自己独有的三个事件 1234567891011121314151617181920212223242526272829303132333435363738val acrobatAdapter = AcrobatAdapter&lt;Int&gt; &#123; itemDSL &#123; resId(R.layout.item_test) showItem &#123; d, pos, view -&gt; view.item_tv.text = &quot;数据Item: &quot; + d &#125; onClick &#123; toastS(&quot;单击&quot;) &#125; onDoubleTap &#123; toastS(&quot;双击&quot;) &#125; longPress &#123; toastS(&quot;长按&quot;) &#125; &#125; itemDSL &#123; resId(R.layout.item_test1) showItem &#123; d, pos, view -&gt; view.item_tv1.text = &quot;另一种样式&quot; + d &#125; isMeetData &#123; d, pos -&gt; pos == 1 &#125; onClick &#123; toastS(&quot;单击另一种Item&quot;) &#125; onDoubleTap &#123; toastS(&quot;双击另一种Item&quot;) &#125; longPress &#123; toastS(&quot;长按另一种Item&quot;) &#125; &#125; &#125;.setData(数据源) 三个事件都是单独绑定Item的样式的！当使用多Item样式列表时，再也不用在click事件中，写很多的条件判断了！ AcrobatAdapter不但支持Item绑定事件，也支持Adapter外部绑定事件。但这样就需要开发者，在外部事件里区分多Item样式了。 适配器持有Item事件，使用AcrobarAdapter的bindEvent()函数 123456789101112131415161718192021val acrobatAdapter = AcrobatAdapter&lt;Int&gt; &#123; itemDSL &#123; resId(R.layout.item_test) showItem &#123; d, pos, view -&gt; view.item_tv.text = &quot;数据Item: &quot; + d &#125; isMeetData &#123; d, pos -&gt; pos != 1 &#125; &#125; &#125;.setData(data).bindEvent &#123; onClick &#123; toastS(&quot;外部单击&quot;) &#125; onDoubleTap &#123; toastS(&quot;外部双击&quot;) &#125; longPress &#123; toastS(&quot;外部长按&quot;) &#125; &#125; 还有几个使用的小Tip ItemDSl的onViewCreated(parent,view)函数 12345678910111213val acrobatAdapter = AcrobatAdapter&lt;Int&gt; &#123; itemDSL &#123; resId(R.layout.item_test) showItem &#123; d, pos, view -&gt; view.item_tv.text = &quot;数据Item: &quot; + d &#125; onViewCreate &#123; parent, view -&gt; 作一些和数据无关的UI操作，譬如view设置为圆形 或者EditText的addTextChangedListener &#125; &#125; &#125; showItem()函数是绑定在适配器的onBingViewHolder函数里的，触发会比较频繁。如果在showItem内做一些UI操作，会比较浪费性能.onViewCreated函数是绑定在适配器的onCreateViewHolder函数内 刷新单个Item布局 Recyclerview如果需要刷新Item的话，不建议使用notifyItemChanged(int position)方法，因为这个方法会刷新整个Item的视图。在视觉上的直观体现就是，Item会闪烁。所以建议使用如下方法刷新Item： 1notifyItemChanged(int position, Object payload) 使用上面这个方法，不会重绘整个View的视图。 12345678910111213val acrobatAdapter = AcrobatAdapter&lt;Int&gt; &#123; itemDSL &#123; resId(R.layout.item_test) showItem &#123; d, pos, view -&gt; view.item_tv.text = &quot;数据Item: &quot; + d &#125; showItemPayload &#123; d, pos, view, payloads -&gt; 刷新Item的某个特定的ChildView。 譬如在某个Item刷新进度条。下面为伪代码 view.progreess_bar.setProgress(100%) &#125; &#125; &#125; 下面简单做一下效果展示： #####使用notifyItemChanged(int position)和showItem刷新布局 #####使用notifyItemChanged(int position, Object payload)和showItemPayload刷新布局 效果对比，一目了然。 结语Kotlin已成为Android开发的官方语言，不管工作上用得到用不到，大家了解一二还是有必要的。毕竟这个时代变化的太快了。以上","path":"2018/07/05/2018-07-05-kotlin-adapter/","date":"07-05","excerpt":"","tags":[{"name":"技术讨论","slug":"技术讨论","permalink":"https://yoursite.com/tags/技术讨论/"}]},{"title":"提供一个Glide灵活加载圆角图片的方法","text":"原创文章，转载请联系作者 前言Glide是目前使用的颇为广泛的图片加载框架，同时也是Google官方推荐使用的。在图片处理方面，它提供了很多不错的功能。 如何才能灵活圆角图片显示，大概是很多APP都会出现的UI设计了，Glide本身也提供了圆角图片的加载方式——但也只是简单的四圆角。实际项目开发中，并不能应付多变的产品需求和善变的UI设计师了。譬如有时候，只需要顶部展示圆角，有时候又只需要左侧展示圆角等等。那么，本次的方法适用于使用了Glide作为项目图片框架的小可爱们，开发者可以对四个圆角进行单独设置，不仅仅是显示隐藏，每个圆角的半径亦是独立存在的。 使用文档Glide有对外暴露一个方法，可以在图片显示前，对图片作转换处理——就是Transformations。有关此方面的文字，小可爱们可以看看这篇——Glide - 自定义转换。本文的RoundCorner就是继承了BitmapTransformation类来实现的。它对外提供两个构造函数，一个构造函数有四个参数，分别是leftTop:左上角、rightTop:右上角、leftBottom:左下角、rightBottom：右下角。可以供外部，灵活的去选择设置哪几个圆角需要去展示，四个圆角的半径大小。另一个函数，只提供一个参数就是同时设置四个圆角，当然这是用于四个圆角同时展示且半径相同的情况下。于此同时，构造函数中只需要传数值即可，类内部已经做了dp处理。下面做一下简单的展示。 加载普通圆角图片 123Glide.with(this).load(&quot;http://p15.qhimg.com/bdm/720_444_0/t01b12dfd7f42342197.jpg&quot;) .apply(RequestOptions.bitmapTransform(RoundCorner(20f))) .into(img) 只是顶部圆角 123Glide.with(this).load(&quot;http://p15.qhimg.com/bdm/720_444_0/t01b12dfd7f42342197.jpg&quot;) .apply(RequestOptions.bitmapTransform(RoundCorner(leftTop = 20f, rightTop = 20f))) .into(img) 只是左侧圆角 123Glide.with(this).load(&quot;http://p15.qhimg.com/bdm/720_444_0/t01b12dfd7f42342197.jpg&quot;) .apply(RequestOptions.bitmapTransform(RoundCorner(leftTop = 20f, leftBottom = 20f))) .into(img) 以上就是一次简单的展示了，如果你想更加灵活的加载圆角图片,选择这个方法没有错。代码在这里，传送门——RoundCorner 结语以上","path":"2018/07/01/2018-07-01-glide-round/","date":"07-01","excerpt":"","tags":[{"name":"技术讨论","slug":"技术讨论","permalink":"https://yoursite.com/tags/技术讨论/"}]},{"title":"提供一种Fragment可见性改变的监测方案","text":"原创文章，转载请联系作者 前言Fragment，这个让人又爱又恨“碎片”。使用它可以让项目更加轻便–我们可以将功能分割、复用，但其复杂的生命周期和Transaction事务，在极端操作【某些测试人员有一手绝活，三指甚至六指同时触屏乱弹】下会出现一些不可预期的错误–Fragment嵌套Fragment,横竖屏切换等等。但无论怎样，面对解决问题，才是关键。这篇文章就是针对Fragment监测可见状态改变，提供一种解决方案。 Fragment可见性解析首先，要说明一下，这里的可见性就是对用户来说看的见。不仅仅是界面位于顶层那种常规情况，而是即便界面上还存在一层透明界面或是对话框，那么依然判定其对用户可见，为visible。接下来会分析在特定交互环境下，Fragment内部被触发的方法。 onResumeFragment是不能单独存在的，它所在的视图树中，往下追溯，根部一定是一个Activity。在源码中，onResume()方法的描述很有意思。 12345678910/** * Called when the fragment is visible to the user and actively running. * This is generally * tied to &#123;@link Activity#onResume() Activity.onResume&#125; of the containing * Activity&apos;s lifecycle. */ @CallSuper public void onResume() &#123; mCalled = true; &#125; 一般情况下，对用户可见时触发。绑定在依赖的Activity生命周期里 也就是说，一般这个方法，会在可见并且正在活跃时被调用。但说到底，还是个“窝里造”，生命周期完全依赖于父容器—-也一定依赖于根Activity。那么不一般的情况下呢？有这么一个例子，在进入一个Activity界面时，直接调用了beginTransaction().hide(Fragment)方法。那么用户一开始就不会看到这个界面，但生命周期确实也走到了onResume。由此可知，可见性的判断不能只依赖于这一个方法的判断。 onHiddenChanged这个方法在使用beginTransaction().hide(Fragment)会被调用，而且是在onResume之前。先来看看源码里的描述。 1234/* @param hidden True if the fragment is now hidden, false otherwise. */ public void onHiddenChanged(boolean hidden) &#123; &#125; 这个方法会回调出来一个参数，true的时候表示隐藏了，false表示可见。在可见性改变时被调用。这里要注意一下这个布尔值的定义！ setUserVisibleHintViewPager搭配Fragment，也是常见的交互模式了。此时左右滑动时，这个方法会被触发。但有一点要说明一下，当ViewPager初始化时，Fragment相应的生命周期里。setUserVisibleHint方法是走在Fragment的onCreate之前的。 以上几个方法，就是常见的交互下，会被触发的方法了。可见性的监测，主要也依赖于这个方法的相互配合。这里还需要说明一下，可见性的监测，监测的是“改变”。也就是当Fragment被创建出来时，不会触发监测方法，不管它是可见还是不可见的状态。 代码实现在BaseFragment内，提供了一个onVisibleToUserChanged(boolean isVisibleToUser)方法作为内部回调。参数isVisibleToUser如字面所示，True表示可见，false不可见。当你需要在界面不可见，取消网络请求或是释放一些东西，你就可以使用此方案。代码实现相当简单，就是一连串逻辑代码而已。只是在onResume方法里，需要判断一下是否已经触发了onHiddenChanged或是setuserVisibleHint方法。代码很短，不到100行。这里直接贴出来。不方便的小可爱们，可以直接去GitHub地址.如果你喜欢的话，不妨点个赞吧。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162abstract class BaseFragment : Fragment()&#123; lateinit var mRootView: View private var isVisibleToUsers = false private var isOnCreateView = false private var isSetUserVisibleHint = false private var isHiddenChanged = false private var isFirstResume = false override fun onCreateView(inflater: LayoutInflater, container: ViewGroup?, savedInstanceState: Bundle?): View? &#123; isOnCreateView = true mRootView = LayoutInflater.from(activity).inflate(getResId(), null, false) return mRootView &#125; abstract fun getResId(): Int override fun onResume() &#123; super.onResume() if (!isHiddenChanged &amp;&amp; !isSetUserVisibleHint) &#123; if (isFirstResume) &#123; setVisibleToUser(true) &#125; &#125; if (isSetUserVisibleHint || (!isFirstResume &amp;&amp; !isHiddenChanged)) &#123; isVisibleToUsers = true &#125; isFirstResume = true &#125; override fun onPause() &#123; super.onPause() isHiddenChanged = false isSetUserVisibleHint = false setVisibleToUser(false) &#125; override fun setUserVisibleHint(isVisibleToUser: Boolean) &#123; super.setUserVisibleHint(isVisibleToUser) isSetUserVisibleHint = true setVisibleToUser(isVisibleToUser) &#125; override fun onHiddenChanged(hidden: Boolean) &#123; super.onHiddenChanged(hidden) isHiddenChanged = true setVisibleToUser(!hidden) &#125; private fun setVisibleToUser(isVisibleToUser: Boolean) &#123; if (!isOnCreateView) &#123; return &#125; if (isVisibleToUser == isVisibleToUsers) &#123; return &#125; isVisibleToUsers = isVisibleToUser onVisibleToUserChanged(isVisibleToUsers) &#125; protected open fun onVisibleToUserChanged(isVisibleToUser: Boolean) &#123; &#125;&#125; 结语以上","path":"2018/06/21/2018-06-21-fragment-v/","date":"06-21","excerpt":"","tags":[{"name":"技术讨论","slug":"技术讨论","permalink":"https://yoursite.com/tags/技术讨论/"}]},{"title":"仿FlipBoard直板翻页效果","text":"原创文章，转载请联系作者 梧桐落，又还秋色，又还寂寞。 效果图,文件比较大，稍稍等一下 (●ﾟωﾟ●)： 前言首先，首先！Demo只是对FliBoard的立体感直板翻页式交互效果作了模仿，只是效果只是效果。那种翻页组件挺麻烦的，以后可能会抽时间做一下(￣▽￣)”立体感是一种模仿，在二维平面上，合理地利用光影、透视（远小近大）等方式，塑造一种近似现实三维世界的感jio。为什么会产生立体感? 是因为人的视网膜接受到的，全是三维世界的投影。是你的大脑以及经验，脑补出了三维世界。举栗子，下面这张图片，你会把它看作一个弯曲三角吗同理，动画也无非是利用了人眼的视觉暂留而已。某种程度，它和魔术拥有相同的本质————欺骗。 效果解析 解析效果前，先提一下会用到的知识点 1、用到的知识点 graphics.Camera，图形包下用来处理3D旋转的类 canvas、Matrix 2、效果拆解直板式的翻页，效果其实并不复杂。手机屏幕之后，是一个三维坐标系。想象一下有张板子(Bitmap)放在XY坐标系，要达到翻页效果，让其绕着X轴旋转即可。正常情况下，板子（Bitmap）是作为整体旋转。我们将板子中心点移到X轴上，那么绕着Z轴旋转时，上下两部分运动的方向肯定是相反的。就像这样： 上图为绕着X轴旋转45度，缩放0.5f效果 如上图所示，为达到效果，必须将上下两部分分开绘制。你可以采用将Bitmap分割的方式，也可以分割Canvas。Demo里，我采取的是分割Canvas。使用方法canvas.clipRect(left, top, right, bottom)。 3、手势拆解翻页共有三种状态，静态、下翻以及上翻。静态不必赘述，下面会分析一下上翻和下翻绘制。 3.1 向下翻页绘制解析向下翻页，就是翻过当前页回到上一页。在效果拆解那部分，我们已经知道，45度时，上半部分会偏向屏幕后。所以要让上半部分向下翻转。旋转角度得是负数。也就是，在一个完整的下翻周期内，角度的变化为0到-180度。其中0到-90度内，当前页正在下翻，页面变动在上半区域，此时可以看到的界面有：下翻ing的当前页上半部分、当前页产生的阴影、上一页的上半部分（保持不动）。而在-90到-180度阶段，此时下翻的动作接近完成，页面变动在下半区域，此时可以看到的界面有：即将翻过的上一页的下半部分、上一页翻转产生的阴影、当前页的下半部分。 3.2 向上翻页绘制解析向上翻页，就是翻过当前页去下一页。和下翻逻辑相反，这是一个0到180度的周期活动。0到90度为正在上翻，页面变动在下半区域。而90到180度，上翻动作接近完成，页面变动在上班区域，很快会看到完整的下一页。 具体实现 用自定义View来实现，这里只贴出主要代码，部分逻辑会用伪代码表述，完整代码文末提供。 1、绘制因为只是仿写效果，所以全部逻辑放在了一个自定义View内部。先看一些主要的成员变量。 1234567891011121314151617181920212223242526272829303132 //向下翻旋转角度,0~-180f private var rotateF //向上翻旋转角度,0~180f private var rotateS //翻动状态，0为松手，1为向下翻，-1为向上翻 private var statusFlip = 0 //当前页 private var curPage//用于3D旋转的Camera类 private val camera//绘制Bitmap的Matrix private val drawMatrix//中心点X坐标 private val centerX//中心点Y坐标 private val centerY //当前Bitmap private var curBitmap: Bitmap //上一张Bitmap private var lastBitmap: Bitmap //下一张Bitmap private var nextBitmap: Bitmap 我维护了两个变量用来分别控制下翻和上翻的角度变化。与此同时，也分了两个方法，来分别绘制上半部分和下半部分。 123456789101112131415161718//上半部分绘制fun drawFirstHalf(canvas: Canvas?, bitmap: Bitmap?, rotate: Float) &#123; canvas?.save() //将canvas上半部分切割 canvas?.clipRect(0, 0, width, height / 2) camera.save() //camera绕着X轴旋转 camera.rotateX(角度变化小于-90度，不再处理) camera.getMatrix(drawMatrix) camera.restore() //随着旋转角度变化的缩放值，只缩放Y轴 drawMatrix.preScale(1.0f, 缩放比) //将图片移到中心点 drawMatrix.preTranslate(-centerX, -centerY) drawMatrix.postTranslate(centerX, centerY) canvas?.drawBitmap(this, drawMatrix, null) canvas?.restore() &#125; 1234567891011121314fun drawSecondHalf(canvas: Canvas?, bitmap: Bitmap?, rotate: Float) &#123; canvas?.save() camera.save() //切割下半部分canvas canvas?.clipRect(0, height / 2, width, height) camera.rotateX(绕着X轴旋转角度，大于90度后只不再处理变化) camera.getMatrix(drawMatrix) camera.restore() drawMatrix.preScale(1.0f, 缩放比随着角度变化) drawMatrix.preTranslate(-centerX, -centerY) drawMatrix.postTranslate(centerX, centerY) canvas?.drawBitmap(this, drawMatrix, null) canvas?.restore() &#125; 2、 手势处理手势处理较为简单，只需要在MOVE的时候，判断此时的状态是上翻还是下翻。然后在抬手UP的时候，根据此时的距离，来判断是否下翻成功或是上翻成功。倘若距离不够标准阈值，那么一切归于原位。 其中startX、startY为手指落点 123456789101112131415161718192021222324252627MotionEvent.ACTION_MOVE -&gt; &#123; val x = this.x val y = this.y //当y运动距离大于x的1.5倍时，才判断为垂直翻动 val disY = y - startY if (Math.abs(disY) &gt; 1f &amp;&amp; Math.abs(disY) &gt;= Math.abs(x - startX) * 1.5f) &#123; if (statusFlip == 0) &#123; //滑动间距为正并且不是第一页判断为向下翻，滑动间距为负并且不是最后一页判断为向上翻 statusFlip = if (disY &gt; 0 &amp;&amp; curPage != 0) DOWN_FLIP else if (disY &lt; 0 &amp;&amp; curPage != girls.lastIndex) UP_FLIP else 0 &#125; val ratio = Math.abs(disY) / centerY if (statusFlip == DOWN_FLIP) &#123; //向下翻并且当前页不等于0 rotateF = ratio * -180f Log.d(&quot;cece&quot;, &quot;: rotateF : &quot; + rotateF); invalidate() &#125; else if (statusFlip == UP_FLIP) &#123; //向上翻，并且不是最后一页 if (curPage != girls.lastIndex) &#123; rotateS = ratio * 180f Log.d(&quot;cece&quot;, &quot;: rotateS : &quot; + rotateS); invalidate() &#125; &#125; &#125; &#125; 当手指抬起时，首先判断此时的状态，然后再判断移动过的距离是否满足阈值。不满足的回归当前页，满足阈值的，继续执行未完成的状态。 1234567891011121314151617181920212223242526272829if (statusFlip != 0) &#123; drawMatrix.reset() //放手的时候，有动画发生 if (Math.abs(event.y - startY) &lt;= centerY / 2) &#123; //滑动距离小于1/4屏幕高，判定仍停留在当前页 rotateF = 0f rotateS = 0f statusFlip = 0 invalidate() &#125; else &#123; //滑动距离超过临界值，判定为跳过当前页 if (statusFlip == DOWN_FLIP) &#123; //自动执行完下翻到上一页的动作 for (i in rotateF.toInt() downTo -180 step 6) &#123; invalidate() &#125; curPage-- &#125; else &#123; //自动执行完上翻到下一页的动作 for (i in rotateS.toInt() until 180 step 6) &#123; invalidate() &#125; curPage++ &#125; rotateF = 0f rotateS = 0f statusFlip = 0 &#125; &#125; 当距离达到阈值时，就需要代码来继续完成下翻或者上翻的逻辑。这里我使用循环的方式。譬如上翻超过90度了，就循环到180度，继续完成上翻的动作。 3、 阴影部分和绘制顺序在onDraw(...)方法内绘制时，一定要注意代码顺序。因为在这个方法内，顺序代表着层次。譬如阴影绘制一定要写在页面绘制之前。阴影部分的绘制也分为上下两部分。 1234567fun drawFirstShadow(canvas: Canvas?, rotate: Float) &#123; canvas切割上半部分，绘制color即可 &#125;fun drawSecondShadow(canvas: Canvas?, rotate: Float) &#123; canvas切割下半部分，绘制color即可 &#125; 在onDraw(...)方法内的绘制顺序一定要分明 12345678910111213141516171819202122232425262728293031//绘制当前页底下的一层,翻页进行中 if (statusFlip == DOWN_FLIP) &#123; //向下翻，滑到上一页 drawFirstHalf(canvas, lastBitmap, 0f) drawFirstShadow(canvas, rotateF) &#125; else if (statusFlip == UP_FLIP) &#123; drawSecondHalf(canvas, nextBitmap, 0f) drawSecondShadow(canvas, rotateS) &#125; //绘制当前页 drawFirstHalf(canvas, curBitmap, rotateF) drawSecondHalf(canvas, curBitmap, rotateS) //绘制当前页之上的一层，翻页完成后 if (statusFlip == DOWN_FLIP) &#123; if (rotateF &lt;= -90f) &#123; //先绘制阴影 drawSecondShadow(canvas, rotateF + 180f) drawSecondHalf(canvas, lastBitmap, rotateF + 180f) &#125; //绘制覆盖在翻页Bitmap之上淡淡透明层，透明度固定 drawFirstColor(canvas, 20) &#125; else if (statusFlip == UP_FLIP) &#123; if (rotateS &gt;= 90f) &#123; drawFirstShadow(canvas, rotateS - 180f) drawFirstHalf(canvas, nextBitmap, rotateS - 180f) &#125; //淡淡透明度的阴影层 drawSecondColor(canvas, 20) &#125; 还是得区分一下状态，当下翻时，我们得先绘制上一页的上半部分，而且是静态的。然后再绘制当前页下翻产生的阴影。再绘制当前页，然后在当前页顶上再绘制一层固定淡淡透明度的阴影层，让页面层次更加明显。 4、效果修正到这里主要的逻辑业已完成，但我注意到还是有一些小瑕疵。就是旋转角度和缩放比，变化不明显。通常要角度变化到超过45度，才会有很明显的缩放效果展现出来。最开始我以为是缩放比的算法问题，后来才发现是camera的机位问题，camera默认的拍摄角度是[0,0,-8]，当距离屏幕很近时，变化自然不是很明显。当然，camera提供了设置机位的方法setLocation(x, y, z)。最后我调整到[0,0,-20]才满意这个效果。 下图，我给出了，默认机位和[0，0，-20]机位的效果区别。 结语Demo里的实现方式并非是唯一，分享出来是为了提供一种思路。路有很多条，选择即是正确。以上项目代码在此，大家要是喜欢的话不妨点个赞吧 有一个公众号，会记录一些开发的经验，也会发一些自己的学习日记。欢迎大家关注。","path":"2018/06/10/2018-06-10-flip/","date":"06-10","excerpt":"","tags":[{"name":"Canvas动画","slug":"Canvas动画","permalink":"https://yoursite.com/tags/Canvas动画/"},{"name":"交互体验","slug":"交互体验","permalink":"https://yoursite.com/tags/交互体验/"}],"preview":"https://raw.githubusercontent.com/JadynAi/oldpage.io/master/img/20180610-blog-header-bg.png"},{"title":"基于Java代码实现的喷漆笔刷","text":"原创文章，转载请联系作者 软草平莎过雨新，轻沙走马路无尘。何时收拾耦耕身？ 先上效果图： 笔刷项目地址在此，大家要是喜欢的话，不妨来点个赞吧 效果解析因为最终要实现的是windwos下的画板喷漆笔刷，所以首先要对它做一个较为详细的效果解析。考虑到笔一般情况下笔刷的使用点，故此会分析一下点和线的效果细节。 画点 从左至右依次是对同一坐标点击2次，点击8次，点击16次的效果展示；当数量趋向更大时，点的密集程度并没有很明显的偏向，基本可以确定要在圆内均匀分布 画线 如图为匀速且缓慢滑过时，由点构成线 具体实现项目的大致框架由View、BasePen，两个大的模块构成。其中View属于UI层面，BasePen属于业务逻辑层面。接下来，将一一介绍这两个模块的具体功用和细节。 View此项目的承载View为PenView，不承担业务逻辑，就是起到一个容器的作用。在PenView中唯一的作用就是触发invalidate（）方法。 1234567891011121314151617181920212223242526272829303132private BasePen mBasePen;@Override protected void onSizeChanged(int w, int h, int oldw, int oldh) &#123; super.onSizeChanged(w, h, oldw, oldh); if (w != 0 &amp;&amp; h != 0) &#123; if (mBasePen == null) &#123; mBasePen = new SprayPen(w, h); &#125; &#125; &#125; @Override public boolean onTouchEvent(MotionEvent event) &#123; MotionEvent event1 = MotionEvent.obtain(event); mBasePen.onTouchEvent(event1); switch (event.getActionMasked()) &#123; case MotionEvent.ACTION_DOWN: case MotionEvent.ACTION_MOVE: invalidate(); break; case MotionEvent.ACTION_UP: break; &#125; return true; &#125; @Override protected void onDraw(Canvas canvas) &#123; super.onDraw(canvas); mBasePen.onDraw(canvas); &#125; 具体的业务逻辑，绘制、数据计算、触摸点移动Move等，全都由BasePen以及它的子类来实现了。低耦合性，代表着更多的自由度，对现有项目代码（如果应用到项目中）的冲击更小。在性能方面，如果View满足不了要求，可以用更小的代价将其移植到性能更好的SurfaceView里去。 业务逻辑业务方面，BasePen作为基类，承担了一些基础的数据计算、绘制等功能，而具体的画笔效果则交由子类实现。先看看BasePen里做了什么： 绘制1234567private List&lt;Point&gt; mPoints;public void onDraw(Canvas canvas) &#123; if (mPoints != null &amp;&amp; !mPoints.isEmpty()) &#123; canvas.drawBitmap(mBitmap, 0, 0, null); drawDetail(canvas); &#125; &#125; 先将笔刷绘制到一张Bitmap之上，再将这张Bitmap交给PenView来绘制出来。Point是一个只记录了x和y坐标的类。drawDetail(Canvas canvas)是一个抽象类，由子类实现具体的绘制。 滑动轨迹在BasePen的onTouchEvent(MotionEvent event1)方法里。以每次DOWN事件为开始，记录MOVE内的所有坐标信息。考虑到喷漆效果基本不用处理笔锋效果，暂不考虑记录UP信息（后续如果实现其他笔刷效果会优化这里）。 12345678910111213141516171819202122232425262728public void onTouchEvent(MotionEvent event1) &#123; switch (event1.getActionMasked()) &#123; case MotionEvent.ACTION_DOWN: clearPoints(); handlePoints(event1); break; case MotionEvent.ACTION_MOVE: handlePoints(event1); break; case MotionEvent.ACTION_UP: break; &#125; &#125; private void handlePoints(MotionEvent event1) &#123; float x = event1.getX(); float y = event1.getY(); if (x &gt; 0 &amp;&amp; y &gt; 0) &#123; mPoints.add(new Point(x, y)); &#125; &#125; private void clearPoints() &#123; if (mPoints == null) &#123; return; &#125; mPoints.clear(); &#125; 喷漆实现 123456789101112131415protected void drawDetail(Canvas canvas) &#123; if (getPoints().isEmpty()) &#123; return; &#125; mTotalNum = 由自定义粒子密度以及画笔宽度计算而来 drawSpray(当前最新坐标点.x, 当前最新坐标点.y, mTotalNum); &#125; private void drawSpray(float x, float y, int totalNum) &#123; for (int i = 0; i &lt; totalNum; i++) &#123; //算法计算出圆内随机点 float[] randomPoint = getRandomPoint(x, y, mPenW, true); mCanvas.drawCircle(randomPoint[0], randomPoint[1], mCricleR, mPaint); &#125; &#125; 以上是一部分伪代码，SprayPen内部定义了一个喷漆粒子密度，会根据画笔的宽度来实时改变粒子数量。每个粒子的半径则由外部依赖的组件提供的width计算而来。在drawDetail(...)方法内，每一次MOVE和DOWN事件都会在相应坐标处，绘制一定数目的圆内随机点。当其串联起来时，就形成了喷漆效果。当然这只是初步完成，还有一些算法需要完善。伪代码表述不全，可参考SprayPen，在代码中有比较完善的注释。 接下来会说一些有关喷漆算法方面的问题。 喷漆算法的几个问题在实现功能的过程中，有两个问题是值得记录的。一是圆内均匀随机点的分布问题；二是滑动速度快时，笔画的连接处理问题。 如何均匀的在圆内生成随机点为了解决这个问题，主要尝试了三种方法： x在(-R,R)范围内随机取值，由圆解析式求解得y。然后对y在(-y,y)内随机取值，得到的点即为圆内点。同理，也可由y计算出x。java代码如下： 12345float x = mRandom.nextInt(r);float y = (float) Math.sqrt(Math.pow(r, 2) - Math.pow(x, 2));y = mRandom.nextInt((int) y);x = 对值随机取正负(x);y = 对值随机取正负(y); 最终呈现效果如下： 当样本数量达到2000时，形状如上所示可以很明显的看到，在x轴方向，左右两端的密集程度明显高于圆心随机值在大量数据下会具有规律性，可以理解为当数据很多时，x的取值在(-r,r)大致为均匀分布的，y的取值亦是。当处于左右两端时，y的取值范围变小，视觉效果就显得紧凑了些。当然如果用概率论数理统计公式来验证会更有说服力，但可惜不会。。。（耸肩） 随机角度，在[0,360)内随机取得角度，然后在[0,r]范围内随机取值，然后使用sin和cos来求解x和y。java代码如下：1234567float[] ints = new float[2];int degree = mRandom.nextInt(360);double curR = mRandom.nextInt(r)+1;float x = (float) (curR * Math.cos(Math.toRadians(degree)));float y = (float) (curR * Math.sin(Math.toRadians(degree)));x = 对值随机取正负(x);y = 对值随机取正负(y); 最终呈现效果如下： 明显看到中心处的密集程度高于边缘地带，事实上当角度固定时，r在[0,R)范围内随机取值。当数量更大时，坐标点是均匀分布的。当r越小时，所占用的面积越小，就会显得粒子很密集。 随机角度，在[0,360)内随机取得角度，取[0,1]内的随机平方根再和R相乘，然后使用sin和cos来求解x和y。java代码如下：123456int degree = mRandom.nextInt(360);double curR = Math.sqrt(mRandom.nextDouble()) * r;float x = (float) (curR * Math.cos(Math.toRadians(degree)));float y = (float) (curR * Math.sin(Math.toRadians(degree)));x = 对值随机取正负(x);y = 对值随机取正负(y); 最终呈现效果如下： 这次的视觉效果总算是达到了均匀的效果，这个算法是利用了一个根函数的特性，如下图:红色是根函数，蓝色是线性函数。两者相比下来，根函数的取值会更大些，相应的，接近边缘的点就会更多一点，让粒子的分布效果更加均衡。 处理“奋笔疾书”情况当以比较慢的速度滑动时，笔画尚显流畅无明显断层。当速度过快时，MOVE留下的点更少，且间距大。会出现画笔断层现象，这时候就需要一些特殊的处理方法。代码中设定了一个标准值D，这个值是由BasePen所持有的w和h两个值计算而来的，一般来说，这两个值期望为依附的View的宽高。最初也考虑使用画笔的直径计算，但考虑到画笔直径是可以外部动态改变的。标准值最好保持一定的独立性，其所依赖的数据越稳定越好，要不然会影响平衡。然后当MOVE时，当前点距离上一个点的相对距离大于这个标准值D时，就会判定此时处于快移速状态，间距越大移速越快，那么喷漆效果相应地就要减弱【直观而言就是粒子浓度要低】。快移速状态时，代码会在当前点和上一个点之间，模拟出一些笔迹点。相应地，这些笔迹点的粒子密集度会低一些，其计算函数且是一个反驼峰的变化状态。即连续笔迹点的中间点粒子最稀疏，两边则最密集。1234567891011121314151617181920212223242526272829303132 //手速过快时float stepDis = mPenR * 1.6f;//笔迹点的数量int v = (int) (getLastDis() / stepDis);float gapX = getPoints().get(getPoints().size() - 1).x - getPoints().get(getPoints().size() - 2).x;float gapY = getPoints().get(getPoints().size() - 1).y - getPoints().get(getPoints().size() - 2).y;//描绘笔迹点for (int i = 1; i &lt;= v; i++) &#123; float x = (float) (getPoints().get(getPoints().size() - 2).x + (gapX * i * stepDis / getLastDis())); float y = (float) (getPoints().get(getPoints().size() - 2).y + (gapY * i * stepDis / getLastDis())); drawSpray(x, y, (int) (mTotalNum * calculate(i, 1, v)), mRandom.nextBoolean()); &#125;/** * 使用（x-（min+max）/2)^2/（min-（min+max）/2）^2作为粒子密度比函数 */ private static float calculate(int index, int min, int max) &#123; float maxProbability = 0.6f; float minProbability = 0.15f; if (max - min + 1 &lt;= 4) &#123; return maxProbability; &#125; int mid = (max + min) / 2; int maxValue = (int) Math.pow(mid - min, 2); float ratio = (float) (Math.pow(index - mid, 2) / maxValue); if (ratio &gt;= maxProbability) &#123; return maxProbability; &#125; else if (ratio &lt;= minProbability) &#123; return minProbability; &#125; else &#123; return ratio; &#125; &#125; Kotlin本项目在写的时候，顺便也写了一个Kotlin版本的。注意，并不是用AS自带的代码转换的。所以Kotlin版本会有很多不必要的测试体验代码，不要在意这些细节。Kotlin版本这里这里，喜欢的不妨点个赞吧 总结以上就是本次Demo的思路、以及一些算法的解析。数学之美，令人沉醉（数学学渣留下了悔恨的泪水。。。）数学才是本体啊笔刷项目地址在此，代码中的注释会更加清晰些，大家要是喜欢的话，不妨来点个赞吧 参考资料： 均匀的生成圆和三角形内的随机点","path":"2018/05/19/2018-05-19-pen/","date":"05-19","excerpt":"","tags":[{"name":"Canvas动画","slug":"Canvas动画","permalink":"https://yoursite.com/tags/Canvas动画/"},{"name":"视觉设计","slug":"视觉设计","permalink":"https://yoursite.com/tags/视觉设计/"}],"preview":"https://raw.githubusercontent.com/JadynAi/oldpage.io/master/img/20180519-blog-header-bg.jpg\""},{"title":"仿ios京东启动页“跳过”效果","text":"天共水，水远与天连天净水平寒月漾，水光月色两相兼 简单展示一下动画效果： 项目地址在此，大家若是喜欢的话，不妨点个赞吧好了，简单阐述一下本次动画的原理： 光的效果使用Paint设置Shader来实现，具体则是LinearGradient水平渐变渲染。光影的平移依赖于LinearGradient的setLocalMatrix，通过Matrix的translate来促使光影移动。 在自定义View的onSizeChanged(int w, int h, int oldw, int oldh)方法内初始化引擎：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Override protected void onSizeChanged(int w, int h, int oldw, int oldh) &#123; super.onSizeChanged(w, h, oldw, oldh); tryInitEngine(w); &#125; private void tryInitEngine(int w) &#123; if (mShadowMatrix == null) &#123; if (w &gt; 0) &#123; //控制阴影的Matrix，通过Matrix的变化来实现闪光的滑过效果 mShadowMatrix = new Matrix(); //因为使用了LinearGradient,所以Paint本身的color将毫无意义，所以colors的起始点的色值必须和本来色值一致 int currentTextColor = getCurrentTextColor(); //渐变色层.x0,y0是起点坐标，x1，y1是终点坐标 mLinearGradient = new LinearGradient(0, 0, 50, 0, new int[] &#123;currentTextColor, Color.GREEN, currentTextColor&#125;, null, Shader.TileMode.CLAMP); //画笔设置Shader getPaint().setShader(mLinearGradient); //使用属性动画作为引擎，数值从-SHADOW变化到TextView本身的宽度。间隔时间未1500ms mValueAnimator = ValueAnimator.ofFloat(-50, w).setDuration(1500); mValueAnimator.setInterpolator(new LinearInterpolator()); mValueAnimator.addUpdateListener(new ValueAnimator.AnimatorUpdateListener() &#123; @Override public void onAnimationUpdate(ValueAnimator animation) &#123; float value = (float) animation.getAnimatedValue(); //Matrix移动来实现闪光滑动 mShadowMatrix.setTranslate(value, 0); invalidate(); &#125; &#125;); mValueAnimator.addListener(new AnimatorListenerAdapter() &#123; @Override public void onAnimationRepeat(Animator animation) &#123; super.onAnimationRepeat(animation); mShadowMatrix.reset(); &#125; @Override public void onAnimationEnd(Animator animation) &#123; super.onAnimationEnd(animation); mShadowMatrix.reset(); &#125; &#125;); mValueAnimator.setRepeatCount(mRepeatCount); &#125; &#125; &#125; 简要说明几个重要变量： mShadowMatrix,用来控制Shader位置的Matrix。 mLinearGradient,实现闪光效果的Shader，水平渐变层。 mValueAnimator，属性动画引擎。 这里面使用了一个LinearGradient线性渐变的着色器。着重说一下它的使用方法。先看一下LinearGradient的构造函数： 1234567891011121314151617 /** Create a shader that draws a linear gradient along a line. @param x0 The x-coordinate for the start of the gradient line @param y0 The y-coordinate for the start of the gradient line @param x1 The x-coordinate for the end of the gradient line @param y1 The y-coordinate for the end of the gradient line @param colors The colors to be distributed along the gradient line @param positions May be null. The relative positions [0..1] of each corresponding color in the colors array. If this is null, the the colors are distributed evenly along the gradient line. @param tile The Shader tiling mode */ public LinearGradient(float x0, float y0, float x1, float y1, int colors[], float positions[], TileMode tile) &#123; ......... ..... ...... &#125; 这其中：x0，y0—-&gt;代表起点的坐标x1，y1—-&gt;代表终点的坐标colors—-&gt;colors表示渲染的颜色，它是一个颜色数组，数组长度必须大于等于2positions—-&gt;positions表示colors数组中几个颜色的相对位置，是一个float类型的数组，该数组的长度必须与colors数组的长度相同。如果这个参数使用null也可以，这时系统会按照梯度线来均匀分配colors数组中的颜色title—-&gt;代表了系统提供的几种渲染模式，这里选用了LinearGradient.TileMode.CLAMP模式，表示重复colors数组里的最后一种颜色直到该View结束的地方 这里我们来做个试验，将colors的最后一个色值改为Color.BLUE 1mLinearGradient = new LinearGradient(0, 0, SHADOW_W, 0, new int[] &#123;currentTextColor, Color.GREEN, Color.BLUE&#125;, OK,看一下试验效果. 可见设置了ClAMP模式后，的确是将colors最后的一个色值覆盖到了未渲染区域 监听属性动画的数值更新，来触发重绘，OnDraw（）方法实现尤为简单。12345678@Override protected void onDraw(Canvas canvas) &#123; super.onDraw(canvas); Log.d(TAG, &quot;onDraw: &quot; + System.currentTimeMillis()); if (mLinearGradient != null) &#123; mLinearGradient.setLocalMatrix(mShadowMatrix); &#125; &#125; Matrix，改变位置即可实现光芒滑过效果 以上就是本次动画的简要原理阐述，项目地址在这里，个中原理已在注释上写的明明白白。FlickerTextView,大家喜欢的话不妨点个赞吧。基于需求的特殊性，本地动画使用了TextView。其实大可不必拘泥于此，本质上还是对Paint设置Shader的使用而已。","path":"2018/04/30/flickerView/","date":"04-30","excerpt":"","tags":[{"name":"Canvas动画","slug":"Canvas动画","permalink":"https://yoursite.com/tags/Canvas动画/"}],"preview":"https://raw.githubusercontent.com/JadynAi/oldpage.io/master/img/20180430-blog-header-bg.jpg"},{"title":"【闻弦歌】--初识音频","text":"学习的目的在于掌握以及更好地使用，学习的第一步就是认知。在音频学习时，我们首先要认知的就是声音。 声音是什么 声音由震动而生，它本质上是一种机械波。既然是波，就拥有波动传播的性质，例如频率、波长等。 对人而言，“声音”于生理学上的体现为大脑接受到的声音，和物理学定义有偏差。人类的耳朵一般只能听到约在20Hz至20,000 Hz（20kHz）范围内的声音，其上限会随年龄增加而降低，蚊音器【注1】就是利用了这种原理。 声音的采集与存储在了解声音采集以及存储之前，我们首先要认知两个概念。即模拟信号和数字信号。信号数据是可以用来表达任何信息的，例如图像、声音、文字等。对信号数据的了解，有利于我们理解声音采集存储的过程中，到底发生了什么。 模拟信号[ analog signal ], [Wiki百科] 是这样解释的： 模拟信号是在时域上数学形式为连续函数的信号，其主要利用对象的一些物理属性来表达传递信息。理论上来说，它的分辨率是接近无穷大的，信息密度大，不存在量化误差。代表着对自然界物理量的描述表达无限逼近真实。同时它也极其容易受干扰。例子：最开始的大哥大传输的就是模拟信号，所以那时候经常出现噪音，通话质量极差。 数字信号[Digital signal]，Wiki百科 解释如下： 数字信号是离散时间信号（discrete-time signal）的数字化表示。 狭义上来说，数字信号就是从模拟信号获取的。 OK，简要的说明了信号数据属性，概念总是苍白无力的，那么接下来我们用一个实际的例子来阐述以上两个属性的含义。就从麦克风录音——&gt;录音文件为例，在这个过程到底发生了什么转换，能让声音变成可播放文件的呢？ 录音解密对着麦克风录音时，麦克风【拾音设备】作了一轮声电转换，将机械振动信号转为模拟信号【在这里模拟信号的体现为连续电信号】。手机【或者计算机】是无法直接处理或存储模拟信号的，因为它是电信号。所以手机【或者计算机】内部会对模拟信号进行采样收集，转化为数字信号，这个过程称之为A/D，模数转换。在这个过程中，采样模拟的波形和原始波形的误差就是噪音了。当然，一定程度上设备的好坏也决定了噪音的程度。By the way，麦克风的工作原理： 麦克风里有一层薄且敏感的碳膜，声音经过时，会压缩空气导致碳膜挤压发生振动。碳膜下方有一个电极，碳膜振动时会接触电极，接触的长短和频率与声波的振幅和频率有关【注2】。这样就完成了第一轮声电转换。 有了录音，自然接下来就是播放了。接着我们会讲一下手机【或者计算机】是如何播放音频文件的。 音频播放 音响设备播放声音，需要连续的电波将音盆的磁圈振动。这个过程，要是向它输入数字信号，那可不好使。音响设备此时需要计算机对它输入模拟信号【有强弱变化的电流】，这个过程叫做D/A,数模转换。模拟信号和数字信号是不相通的，通过模数转换/数模转换来互通。譬如手机播放一个MP3，必须将这个MP3文件解码成模拟信号才能推动扬声器来振动产生声音。 好比是乐谱和演奏一样：A/D，模数转换就是将音乐写成乐谱。而D/A,数模转换，就是乐师将乐谱演奏出来。 音频学习学什么实际上对于APP应用层开发而言，A/D，模数转换和D/A,数模转换大多数情况下是不用接触的。对于Android平台而言，系统已经封装了相当完善的录音Api。它会直接提供给我们，经过A/D模数变换后的二进制序列 PCM文件 【该文件没有附加的文件头和文件结束标志】。而音频开发都是基于PCM文件的。 PCM文件PCM文件是对声音模拟信号的量化，是没有经过压缩的纯音频数据，只有PCM能直接进行声音处理【譬如变声器】。同理，PCM没有文件描述，所以不会有播放器支持播放PCM文件的音乐，除非已知采样率等信息。PCM作为声音的量化体现，主要有以下几个维度来描述声音： 采样率sampleRate，单位【khz】： 取样频率，每秒钟取得声音样本的次数。此值越高，则声音的还原度越高，同时占用的资源也会更多。 常见的有8khz【电话等使用】、22.05khz【广播】、44.1khz【CD音质】、48khz【数字电视】；常用采样率不会超过48khz 44.1khz是Android平台已知唯一兼容大多数固件的采样率 采样位数，单位【bit】： 每一个采样点的采样值，用来衡量声音波动变化的一个参数，也可以说是声卡的分辨率。数值越大，声音质量越好。 大小常为为4bit、8bit、16bit、32bit 声道数channel： 单声道 Mono 双声道 Stereo 用耳机举例，单声道并不是只有一个耳机喇叭发出声音。而是两个喇叭同时发出这个声道的声音。双声道就是两个耳机喇叭分别播出两个声道的声音。 对Android平台而言，立体声就是双声道。 时间 PCM文件计算大小衡量PCM文件数据单位时间内的容量大小，称之为比特率——即1s时间内的比特（bit）数目。那么，来计算一下一个 44.1khz 16bit 2channel的CD音质的比特率为： 144100*16*2=1411200bps=1411.2kbps 那么一段60s的CD音质的声音大小为：11411.2*60/8=10584KB=10.34MB ok，接下来我们详细介绍一下，以上各个单位的换算详情。 比特率，数据传送速度单位,用来衡量带宽的，每s传输的二进制位数： Mbps 即 Milionbit pro second(百万位每秒)Kbps 即 Kilobit pro second（千位每秒）bps 即 bit pro second（位每秒）最小的单位就是bit，也是采样位数，通常缩写为b。这里要着重注意一下，要和另一个单位B区分开来。另，kbps和bps的换算单位也有争议，有的是1kbps=1024bps，有的则是1kbps=1000bps。我们之前计算的10.34MB，是以1000位基准计算的，参考的是wiki百科。 传输的字节数单位，Byte，通常用B表示： 这个才是通常软件上显示的下载速度，也是存储的硬盘上的字节数体现。MB即百万字节也称兆字节 KB即千字节B即字节,其换算单位为1024，公式为：1MB=1024KB=1024*1024B=1024*1024*8b所以，一般1M的网络带宽指的是：1Mbps。它下载的速度上限为：1Mbps=1000kbps=1000000bps。一秒钟走过了10^6个bit，那么换算为Bytes为： 110^6/8=125000 byte/s=122 KB/s PS：有关1kbps=1024bps和1kbps=1000bps的用法依然是混乱的。作为万国共通的SI单位系「k」「M」等接头辞有着1000乘方的意义，IEEE和IEC等学会标准化团体等也正式地跟随这些的用法。但出于计算机初期软件和硬件设计上的情况，是把1024比特作为1000比特，结果在数据的容量和通信速率等中适用这个规则的人增多，导致了现在这样的混乱状态。在通讯的世界中，由于数据通讯开始之前就遵从SI单位系而使用1000倍，并且调制解调器的通信速率开始时也是使用75bps，300bps，1200bps等非2乘方的值，所以一般认为1kbps=1000bps是妥当的。大致来说，存储器和硬盘等存储容量一般使用「1024」，通信速率使用「1000」,但因为根据状况会有不同，需要充分注意使用的是哪边的意义 PCM文件格式可以直接作声音处理对一个PCM文件而言，以采样频率作为变量，其余属性不变。则采样频率越小，播放出来的声音越粗，速度越慢。采样频率越高，声音越细，速度越快。怎么样，是不是很熟悉呢？这种效果差不多就是变声器的原理了，不过变声器要比这个复杂的更多。 以上 注1：蚊音器，也叫蚊音警报器，是一种通过发出高频声音来制止年轻人集会的一种电器设备。最新的版本于2008年晚些时候上市，其可以设置两种频段。一种是大约17.4kHz[1]，一般只有年轻人可以听到的。另一种为8kHz，能被大多数人听到。其最大的前在输出的声压级为108dB。由于人耳朵的功能会随着年纪变化，因此此种声音只有差不多25岁以下的年轻人才听得到。注2：参考《音视频开发进阶指南》第一张第二节 参考资料： Mbps、Kbps、bps、kb、mb的区别，带宽换算 AD/DA转换、模拟、数字的扫盲贴","path":"2018/04/02/audio_start_learn/","date":"04-02","excerpt":"","tags":[{"name":"科普向","slug":"科普向","permalink":"https://yoursite.com/tags/科普向/"},{"name":"音频","slug":"音频","permalink":"https://yoursite.com/tags/音频/"}],"preview":"https://raw.githubusercontent.com/JadynAi/oldpage.io/master/img/20180402-blog-header-bg.jpg"},{"title":"吐血整理！17年下半年面试问题以及解析精华整理","text":"前段时间有一篇面试总结，这里是原文原文,奈何只给了考卷没有答案。最近又处于金三银四，正是跳槽加薪的大好良机啊。便抽了时间将答案整理了一下。本文章大部分内容整理自网络，Po出来的网页也是经过了一轮筛选的，可读性比较好一点。另，一些主观性比较强的问题就添加了一丢丢个人看法，大家尽兴阅读。 本文系个人辛苦整理收纳，转载请注明出处。 2017Android面试总结之答案篇 LRUCache原理 LruCache中维护了一个集合LinkedHashMap，该LinkedHashMap是以访问顺序（accessOrder为true，其余非构造函数此值全为false）排序的。当调用put()方法时，就会在集合中添加元素，并调用trimToSize()判断缓存是否已满，如果满了就用LinkedHashMap的迭代器删除队尾元素，即近期最少访问的元素。当调用get()方法访问缓存对象时，就会调用LinkedHashMap的get()方法获得对应集合元素，同时会更新该元素到队头。 LinkedHashMap内部是使用双向循环链表来存储数据的。也就是每一个元素都持有他上一个元素的地址和下一个元素的地址。 图片加载原理（Glide） 这篇文章对几大开源图片加载库做了对比Android 三大图片缓存原理、特性对比 个人使用过ImageLoader和Glide，相对而言对Glide较为熟悉。对Glide这个点可以稍作分析，如何感知activity或者Fragment生命周期的： 往Activity或者Fragment内部添加一个SupportRequestManagerFragment透明的Fragment，用回调的方式感知生命周期。 模块化的好处 模块化的好处以及原因 JVM JVM的工作原理 总结的JVM面试题 视频加密传输 加密： DES对整个视频文件进行加密，但耗时较长。 将视频数据流前n个字节（n&gt;=2）打乱即可达到加密目的，使用到了内存映射文件（对RandomAccessFile和MappedByteBuffer的使用）。参考这篇文章 数据加密传输： android上数据加密传输比较麻烦，得不偿失。综合阅读过的几篇文章，觉得还是先加密再传输来得好。 统计启动时长，标准 参考文章 app冷启动的流程如下：12345678910-&gt; Application 构造函数-&gt; Application.attachBaseContext()-&gt; Application.onCreate()-&gt; Activity 构造函数-&gt; Activity.setTheme()-&gt; Activity.onCreate()-&gt; Activity.onStart-&gt; Activity.onResume-&gt; Activity.onAttachedToWindow-&gt; Activity.onWindowFocusChanged 如何保持应用的稳定性 经验之谈：性能分析，内存监测，Monkey ThreadLocal 用处：针对线程操作对象，其余线程不能进行操作 原理：ThreadLocal的操作都是针对当前线程持有的一个ThreadLocalMap对象，其内部维护了一个Entry[]（弱引用）数组。所以不同线程操作同一个ThreadLocal对象，内部说白了都是操作各个线程自己的ThreadLocalMap对象而已。 谈谈classloader 主观性比较强，谈一下自己了解的即可。可参考classloader使用与原理分析 动态布局 代码写布局，无他唯手熟尔 一篇干货分享一下 热修复，插件化 主观性较强，视个人了解程度而区分。参考 HashMap源码，SparseArray原理 HashMap实现原理 简洁对比HashMap和SparseArray 应用启动优化 参考Android性能优化 视觉优化，启动界面设置为特殊样式 异步初始化组件； 梳理业务逻辑，延迟初始化组件、操作； 正确使用线程； 去掉无用代码、重复逻辑等； 怎么去除重复代码 抽取成方法，抽取成对象，抽取成Module； 不要过度设计，及时重构，代码要经常回顾； SP是进程同步的吗?有什么方法做到同步 能同步但不建议使用，使用ContentProvider可以做到同步。ContentProvider多进程共享SP数据 SurfaceView介绍 View适用于主动更新的情况，而SurfaceView则适用于被动更新的情况，比如频繁刷新界面。 View在主线程中对页面进行刷新，而SurfaceView则开启一个子线程来对页面进行刷新。 View在绘图时没有实现双缓冲机制，SurfaceView在底层机制中就实现了双缓冲机制 HashMap实现原理，ConcurrentHashMap 的实现原理 参考此篇文章 BroadcastReceiver，LocalBroadcastReceiver 区别 应用场景 BroadcastReceiver用于应用之间的传递消息； 而LocalBroadcastManager用于应用内部传递消息，比broadcastReceiver更加高效 安全 BroadcastReceiver使用的Content API，所以本质上它是跨应用的，所以在使用它时必须要考虑到不要被别的应用滥用 LocalBroadcastManager不需要考虑安全问题，因为它只在应用内部有效 Bundle、Handler、事件传递机制，都是基础 线程间操作List App启动流程，从点击桌面开始 这一篇文章总结的好一点，App启动流程 动态加载（插件化技术） 主观性较强，视个人经历的项目而定。个人觉得如果简单介绍的话，这篇文章会好一点。清晰简洁动态加载简单易懂的介绍方式 GC回收机制 新生代和旧生代采用不同的垃圾回收机制 可参考Java垃圾回收机制 画出Android大体架构图 经典图 点击 Android Studio 的 build 按钮后发生了什么 Android使用gradle构建生成的apk关键就是aapt处理资源文件，aidl处理.aidl，javac生成.class文件，proguard混淆后再由dex生成.dex文件，由apkbuilder签名后再经zipalign对齐字节码就可以上线发布了 这篇文章分析的较为详细，AS的build做了什么 一个应用程序安装到手机上时发生了什么 安装和卸载都是通过PackageManager，实质上是实现了PackageManager的远程服务PackageManagerService来完成具体的操作，所有细节和逻辑均可以在PackageManagerService中跟踪查看； 所有安装方式殊途同归，最终就回到PackageManagerService中，然后调用底层本地代码的installd来完成。再看apk的安装过程。 拷贝apk文件到指定目录 解压apk，拷贝文件，创建应用的数据目录 解析apk的AndroidManifinest.xml文件 向Launcher应用申请添加创建快捷方式 深入探究apk安装过程 对 Dalvik、ART 虚拟机有基本的了解 Dalvik虚拟机执行的是dex字节码，ART虚拟机执行的是本地机器码 可参考这篇文章JAVA虚拟机、Dalvik虚拟机和ART虚拟机简要对比 Android 上的 Inter-Process-Communication 跨进程通信时如何工作的 跨进程通信主要靠Binder详解Binder Android中App 是如何沙箱化的,为何要这么做 Android中的沙箱化可以提升系统安全性和效率 权限管理系统 Android权限机制 进程以及Application生命周期 进程生命周期 有关Application Recyclerview和ListView对比 相信目前大部分Android开发已经舍弃了ListView而转向Recyclerview的怀抱了吧，无需赘言。 快速排序以及B+树，算法算是我比较薄弱的环节，不敢多言 TCP和UDP的区别 TCP是面向连接的协议，提供稳定的双向通信功能，本身提供超时重连机制。UDP是无连接的，提供不稳定的单向通信功能 synchronized与Lock的区别 从用法、性能、用途来分析，很不错。深入研究 Java Synchronize 和 Lock 的区别与用法 volatile 排他锁，保证了所有线程看到的变量都是一致的 Java线程池 java对象生命周期 经典图例 双亲委派模型 老油条式面试题，这篇文章有一个比较新颖的疑问解答关于Java类加载双亲委派机制的思考（附一道面试题） Android事件分发机制 基础的基础 MVP模式 组件和业务逻辑相互分离，两不相知。Google提供的Demo里，View层级真是纯是view操作，包括Listener的一部分都由Presenter承担了。 接口定义清晰明朗 视个人项目经历而谈，详细了解就仔细说，不太懂就说个大概 RxJava 开发神器，错误处理、线程调度、写出来的代码优雅。 可从线程调度以及源码角度剖析剖析。 抽象类和接口的区别 属性和行为的区别 Android消息机制 进程调度 进程与线程 进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。 另外，进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率 死锁 老问题，Java 实例 - 死锁及解决方法 进程状态 图 JVM内存模型 Java 常用并发集合有哪些 非阻塞式列表对应的实现类：ConcurrentLinkedDeque 阻塞式列表对应的实现类：LinkedBlockingDeque 用于数据生成或者消费的阻塞式列表对应的实现类：LinkedTransferQueue 按优先级排序列表元素的阻塞式列表对应的实现类：PriorityBlockingQueue 带有延迟列表元素的阻塞式列表对应的实现类：DelayQueue 非阻塞式列表可遍历映射对应的饿实现类：ConcurrentSkipListMap 随机数字对应的实现类：ThreadLockRandom 原子变量对应的实现类：AtomicLong和AtomicIntegerArray ConcurrentHashMap 的实现原理 Java线程run和start方式的区别 常见的数据结构 如图 堆排序实现 链表反转 synchronized用法 写的较为详细，最后的总结面试时可以直接拿来用Java中Synchronized的用法 OkHttp是如何处理缓存的 OkHttp缓存处理 OkHttp还是使用的Http缓存，需要后台配合。当然也可以单纯的在客户端作拦截处理 bitmap如何处理大图，如何预防OOM 老生常谈的问题，彻底了解 bitmap 高效加载 进程保活 黑科技保活 listview图片加载错乱的原理和解决方案 郭霖大神的解析，拳拳到肉ListView异步加载图片乱序问题，原因分析及解决方案 广播小结 service生命周期 多线程 用RxJava的话会方便很多。RxJava中的多线程，这篇文章真的太赞了 AsyncTask 缺陷，一部分是使用不当，一部分是其本身存在的缺陷。网上的大部分文章都是看了一遍其实都是使用的问题，推荐看看这篇详细解读AsyncTask的黑暗面以及一种替代方案【By，AsyncTask真的很难用，因为要注意的地方太多了😒】 数据库数据迁移，把大象装进新冰箱共分几步？ 将表名改成临时表ALTER TABLE Order RENAME TO _Order 创建新表CREATETABLE Test(Id VARCHAR(32) PRIMARY KEY ,CustomName VARCHAR(32) NOTNULL , Country VARCHAR(16) NOTNULL) 导入数据INSERTINTO Order SELECT id, “”, Age FROM _Order 删除临时表DROPTABLE _Order 设计模式 自由发挥题目。但亦有文章可以参考，这是一个有关Android中常见设计模式解析的一个专栏，推荐指数五颗星Android常见设计模式 Java注解 EventBus、Retrofit等都用到了注解。是一个很强大的东西，关键是代码写起来敲好看。深入理解Java注解类型(@Annotation) Android优化 自由度很高的问题，随意发挥。参考这个优化系列的文章Android 性能优化的方方面面 EventBus实现原理 相比起来，还是更喜欢RxJava自己实现一套Rxbus。EventBus 3.0 源码分析","path":"2018/03/10/2018_face_book/","date":"03-10","excerpt":"","tags":[{"name":"开发经验","slug":"开发经验","permalink":"https://yoursite.com/tags/开发经验/"}],"preview":"https://raw.githubusercontent.com/JadynAi/oldpage.io/master/img/post-bg-2015.jpg"},{"title":"文字动次打次，让文字动态绘制出来","text":"本文属于Android技术论述文章，阅读完大致需要五分钟 原创文章，转载请注明出处。 没时间的小伙伴可以直接跳过文章，点击项目地址，如果喜欢的话，顺手给个star那是极好的【娇羞……】 二话不说，效果奉上！ 知识点序列： Canvas绘制 TextPaint Path PathMeasure【测量Path】 代码分析获取文字的轮廓Path在TextPaint中有这样一个方法，getTextPath,如下： 12345678public void getTextPath(String text, int start, int end, float x, float y, Path path) &#123; if ((start | end | (end - start) | (text.length() - end)) &lt; 0) &#123; throw new IndexOutOfBoundsException(); &#125; nGetTextPath(mNativePaint, mNativeTypeface, mBidiFlags, text, start, end, x, y, path.mutateNI()); &#125; 👀其中nGetTextPath里面是调用了一个原生函数，具体实现不必理会。只需要了解函数的各个参数就可以了。 参数含义： text：文本内容 start：需要测量的文本中的第一个字符的下标 end：需要测量的文本最后一个字符的下标加1 x：Path起点的坐标X y：Path起点的坐标Y path：最终测量得到的Path Path分段绘制PathMeasure这个类可以说是Path相关的工具解析类，里面大多是原生函数，实现细节不必深究。只需要会使用方法即可。第一步我们得到文字的轮廓Path之后，这一步将此path按段绘制出来即可。在代码中，由CanvasView这个类来具体实现绘制的详尽流程。看代码，将文本轮廓Path设置进来： 123456789101112131415161718// 保留一份原始Path，用作绘制最终填色的文字 mOrignalPath = orignalPath; if (null == mPathMeasure) &#123; mPathMeasure = new PathMeasure(); &#125; //先重置一下需要显示动画的path mAnimPath.reset(); mAnimPath.moveTo(0, 0); mPathMeasure.setPath(orignalPath, false); // getLength（）方法获得的是当前path的长度；而nextContour（）方法是将Path切换到下一段Path，多应用在复杂path中 mTextCount = 0; // 计算文字总共有多少段Path while (mPathMeasure.nextContour()) &#123; mTextCount++; &#125; // PathMeasure重新设置一次 mPathMeasure.setPath(orignalPath, false); mPaint.setStyle(Paint.Style.STROKE); 其中成员变量mAnimPath，即是用在onDraw()方法内用于绘制的Path。我们会不停地通过PathMeasure刷新这个Path，用于动画流畅运行。 接下来是引擎代码，使用的是属性动画，看代码实现： 1234567891011121314151617181920212223242526272829if (null == mValueAnimator) &#123; // 如果一个文本Path包含n个小Path，那么属性动画会Repeat运行n次，每一段小path默认动画时间为900ms mValueAnimator = ValueAnimator.ofFloat(0.0f, 1.0f); mValueAnimator.setDuration(900); mValueAnimator.setInterpolator(new LinearInterpolator()); &#125; // 引擎无限次重复发动 mValueAnimator.setRepeatCount(ValueAnimator.INFINITE); mValueAnimator.addUpdateListener(new ValueAnimator.AnimatorUpdateListener() &#123; @Override public void onAnimationUpdate(ValueAnimator animation) &#123; float value = (float) animation.getAnimatedValue(); // 将一段小Path从0%到100%赋值到mAnimPath中，调用重绘 mPathMeasure.getSegment(0, mPathMeasure.getLength() * value, mAnimPath, true); invalidate(); &#125; &#125;); mValueAnimator.addListener(new AnimatorListenerAdapter() &#123; @Override public void onAnimationRepeat(Animator animation) &#123; super.onAnimationRepeat(animation); //绘制完一条Path之后，再绘制下一条，直到完成为止。 if (!mPathMeasure.nextContour()) &#123; animation.end(); &#125; invalidate(); &#125; &#125;); 注释里具体原因亦已写明。这种引擎的设置，就是每一小段Path不管长短，其绘制时间都是相等的，会造成动画看起来时慢时快。 其实要让动画一直匀速跑起来也很容易，就是提前将原始Path测量一遍，设置一个总时间，然后根据小Path的长短来按照比例分配时间。这样即可使动画匀速进行。具体代码不再多言😘，有兴趣的小伙伴可以试试。 再看看CanvasView内的onDraw()方法实现： 12345678910@Override protected void onDraw(Canvas canvas) &#123; super.onDraw(canvas); if (null != mPathMeasure &amp;&amp; mPathMeasure.getLength() == 0) &#123; mPaint.setStyle(Paint.Style.FILL); canvas.drawPath(mOrignalPath, mPaint); return; &#125; canvas.drawPath(mAnimPath, mPaint); &#125; 这里面的代码更加简单，在Path绘制完之前，一直绘制mAnimPath即可。Path绘制完之后，设置画笔的绘制风格，将文字空白处填上色即可。 以上。就是本次项目的主要思路解析。大家走过路过不要错过，给添个Star呗😄。","path":"2017/11/02/anim_text/","date":"11-02","excerpt":"","tags":[{"name":"Canvas动画","slug":"Canvas动画","permalink":"https://yoursite.com/tags/Canvas动画/"}],"preview":"https://raw.githubusercontent.com/JadynAi/oldpage.io/master/img/20170819-blog-header-bg.jpg"},{"title":"实现薄荷Loading动画","text":"本文属于Android技术论述文章，阅读完大致需要五分钟 原创文章，转载请注明出处。 没时间的小伙伴可以直接跳过文章，点击项目地址，如果喜欢的话，顺手给个star那是极好的【娇羞……】 好吧。先说一下为什么要做这个项目。前几天使用薄荷的时候，凑巧看到了这个Loading动画，觉得效果还不错。就想尝试着实现一下。先看一下原版的效果，GIF录制的比较快，但应该还可以看清楚。 先上本次最终实现的效果图吧，颜色当然选择今年最流行的原谅色： 思路分析 1、整个图形的形状如何绘制 2、如何让线条动起来 整个图形的形状分析 好了，首先我们来分析一下这个图案，如果是静态的，那么如何绘制？很简单，拆分。我们将图形拆开分解，然后再看。分析细节和步骤，这是要点。我这里将这个图分成了三份。 第一个，也就是叶柄。也就是下面那一条小小的竖线。原Loading图中不甚明显，但还是有的。叶柄没什么说的，直线就可以了。 第二个，叶子的左轮廓边缘和右轮廓边缘。这是一段下肥上窄的弧线，椭圆截取感觉不妥，我这里采用的是贝塞尔二阶曲线。有关Android贝塞尔相关的知识大家可以看看这篇文章。 第三个，也就是叶片的脉络，线和线交叉连接，没什么可说的。 那么重点其实就是叶子左右轮廓的绘制了，我画了一张草图。大家可以看看： 其中黑色的框作为View的边界。A点是左轮廓曲线的起点，B点事贝塞尔曲线的控制点，我把它定义到了View的左边框那里。C点事整个贝塞尔曲线的终点，D点则是实际上曲线的最高点。右轮廓则和左轮廓是镜像存在。图有点潦草，不过应该还看得懂。 好了，静态图形拆解完毕。接着看，如何让图动起来。 如何让线条动起来整个项目中，如何让线条真正的动起来才是要点。刚开始在这里的思路，是想使用canvas.drawCircle绘制在一张Bitmap上，以点汇面。后面实现起来发现，这种方式特别不靠谱。为什么不靠谱呢？因为点连接成线，每次移动的速率和距离都得计算，很麻烦。很容易出现断点的情况。最后，我采用的是让canvas去绘制一段Path路径，然后Path路径不停的刷新改变。这样做的好处，是Path更加直观易于控制。而且还不用多绘制一张Bitmap。整个项目中，自定义的View，LeafAnimView做的工作很少，只是在onDraw方法内，调起了绘制而已。具体的绘制都交给LeafAtom了。面向对象嘛。 具体的思路，是我把总时间按比例分成四部分。生成四个属性动画，在属性动画的监听里作Path的x和y的变化。在绘制的时候，只需要将这四个动画依次播放，即可得到每个时间段的具体运动值。而且还是均匀变化的。 LeafAnimView内部作为动画引擎的是一个ValueAnimator,使用它来触发View的onDraw。同时也使用它来控制整个动画的时间。 123456789mValueAnimator = ValueAnimator.ofFloat(0, 1); mValueAnimator.setDuration(5000); mValueAnimator.setRepeatCount(ValueAnimator.INFINITE); mValueAnimator.addUpdateListener(new ValueAnimator.AnimatorUpdateListener() &#123; @Override public void onAnimationUpdate(ValueAnimator animation) &#123; invalidate(); &#125; &#125;); LeafAtom类内部接受到这个总时长，然后将运动总时间分割，根据比例计算出绘制叶柄、左右轮廓、脉络的动画时间。 123456789101112131415161718192021222324252627282930313233343536-------在LeafAnimView类内部--------- @Override protected void onDraw(Canvas canvas) &#123; super.onDraw(canvas); if (null == mLeafAtom) &#123; //传入总时长 mLeafAtom = new LeafAtom(getWidth(), getHeight(), mValueAnimator.getDuration()); &#125; if (!mValueAnimator.isStarted()) &#123; mValueAnimator.start(); &#125; //开始绘制 mLeafAtom.drawGraph(canvas, mPaint); &#125; -------在LeafAtome类---------------public static final float PETIOLE_RATIO = 0.1f;//叶柄所占比例public LeafAtom(int width, int height, long duration) &#123; mWidth = width; mHeight = height; mPetioleTime = (long) (duration * PETIOLE_RATIO);//绘制叶柄的时间 mArcTime = (long) (duration * (1 - PETIOLE_RATIO) * 0.4f);//左右轮廓弧线的时间 mLastLineTime = duration - mPetioleTime - mArcTime * 2;//最后一段叶脉的时间 mBezierBottom = new PointF(mWidth * 0.5f, mHeight * (1 - PETIOLE_RATIO));//左侧轮廓底部点 mBezierControl = new PointF(0, mHeight * (1 - 3 * PETIOLE_RATIO));//左侧轮廓控制点 mBezierTop = new PointF(mWidth * 0.5f, 0);//左侧轮廓顶部结束点 mVeinBottomY = mHeight * (1 - PETIOLE_RATIO) - 10;//右侧轮廓底部点Y轴坐标，稍稍低一点 mOneNodeY = mVeinBottomY * 4 / 5;//第一个节点的Y轴坐标 mTwoNodeY = mVeinBottomY * 2 / 5;//第二个节点Y轴坐标 initEngine(); setOrginalStatus(); &#125; 在LeafAtom的构造函数中，得到每一个阶段动画的时间，然后生成四个属性动画，在这个属性动画的监听里去做Path的x和y坐标的值变化。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374/** * 初始化path引擎 */ private void initEngine() &#123; //叶柄动画，Y轴变化由底部运动到叶柄高度的地方 mPetioleAnim = ValueAnimator.ofFloat(mHeight, mHeight * (1 - PETIOLE_RATIO)).setDuration(mPetioleTime); //左右轮廓贝塞尔曲线，只需要只奥时间变化是从0~1的。起点、控制点、结束点都知道了 mArcAnim = ValueAnimator.ofFloat(0, 1.0f).setDuration(mArcTime); //绘制叶脉的动画 mLastAnim = ValueAnimator.ofFloat(mVeinBottomY, 0).setDuration(mLastLineTime); mPetioleAnim.setInterpolator(new LinearInterpolator()); mArcAnim.setInterpolator(new LinearInterpolator()); mLastAnim.setInterpolator(new LinearInterpolator()); mArcRightAnim = mArcAnim.clone(); mPetioleAnim.addUpdateListener(new ValueAnimator.AnimatorUpdateListener() &#123; @Override public void onAnimationUpdate(ValueAnimator animation) &#123; mY = (float) animation.getAnimatedValue(); &#125; &#125;); mArcAnim.addUpdateListener(new ValueAnimator.AnimatorUpdateListener() &#123; @Override public void onAnimationUpdate(ValueAnimator animation) &#123; computeArcPointF(animation, true); &#125; &#125;); mArcRightAnim.addUpdateListener(new ValueAnimator.AnimatorUpdateListener() &#123; @Override public void onAnimationUpdate(ValueAnimator animation) &#123; computeArcPointF(animation, false); &#125; &#125;); mLastAnim.addUpdateListener(new ValueAnimator.AnimatorUpdateListener() &#123; @Override public void onAnimationUpdate(ValueAnimator animation) &#123; mY = (float) animation.getAnimatedValue(); float tan = (float) Math.tan(Math.toRadians(30)); if (mY &lt;= mOneNodeY &amp;&amp; mY &gt; mTwoNodeY) &#123; mOneLpath.moveTo(mX, mOneNodeY); mOneRpath.moveTo(mX, mOneNodeY); //这里的参数x和y代表相对当前位置偏移量，y轴不加偏移量会空一截出来，这里的15是经验值 mMainPath.addPath(mOneLpath, 0, EXPRIENCE_OFFSET); mMainPath.addPath(mOneRpath, 0, EXPRIENCE_OFFSET); //第一个节点和第二个节点之间 float gapY = mOneNodeY - mY; mOneLpath.rLineTo(-gapY * tan, -gapY); mOneRpath.lineTo(mX + gapY * tan, mY); &#125; else if (mY &lt;= mTwoNodeY) &#123; mTwoLpath.moveTo(mX, mTwoNodeY); mTwoRpath.moveTo(mX, mTwoNodeY); //第二个节点，为避免线超出叶子，取此时差值的一半作计算 float gapY = (mTwoNodeY - mY) * 0.5f; mMainPath.addPath(mTwoLpath, 0, EXPRIENCE_OFFSET); mMainPath.addPath(mTwoRpath, 0, EXPRIENCE_OFFSET); mTwoLpath.rLineTo(-gapY * tan, -gapY); mTwoRpath.rLineTo(gapY * tan, -gapY); &#125; &#125; &#125;); mEngine = new AnimatorSet(); mEngine.playSequentially(mPetioleAnim, mArcAnim, mArcRightAnim, mLastAnim); mEngine.addListener(new AnimatorListenerAdapter() &#123; @Override public void onAnimationEnd(Animator animation) &#123; super.onAnimationEnd(animation); setOrginalStatus(); &#125; &#125;); &#125; 计算贝塞尔曲线运动过程中的方法。贝塞尔曲线是有一个函数的，我们知道起点、控制点、终点的话，就可以根据时间计算出此时此刻的x和y的坐标。而这个时间变化是从0~1变化的。谨记。 1234567891011121314151617private void computeArcPointF(ValueAnimator animation, boolean isLeft) &#123; float ratio = (float) animation.getAnimatedValue(); //ratio从0~1变化，左右轮廓三个点不一样 PointF bezierStart = isLeft ? mBezierBottom : mBezierTop; PointF bezierControl = isLeft ? mBezierControl : new PointF(mWidth, mHeight * (1 - 3 * PETIOLE_RATIO)); PointF bezierEnd = isLeft ? mBezierTop : new PointF(mWidth * 0.5f, mVeinBottomY); PointF pointF = calculateCurPoint(ratio, bezierStart, bezierControl, bezierEnd); mX = pointF.x; mY = pointF.y; &#125; private PointF calculateCurPoint(float t, PointF p0, PointF p1, PointF p2) &#123; PointF point = new PointF(); float temp = 1 - t; point.x = temp * temp * p0.x + 2 * t * temp * p1.x + t * t * p2.x; point.y = temp * temp * p0.y + 2 * t * temp * p1.y + t * t * p2.y; return point; &#125; 叶脉的绘制，在节点一和节点二，分别加上两个向左和向右伸展开的Path路径即可。 需要说明的是，lineTo和rLineTo的区别，lineTo的参数代表的就是目标参数，而rLineTo的参数代表的是，目标参数和起点参数的差值。 最后在drawGraph函数中，启动这个动画集合： 12345678public void drawGraph(Canvas canvas, Paint paint) &#123; if (mEngine.isStarted()) &#123; canvas.drawPath(mMainPath, paint); mMainPath.lineTo(mX, mY); &#125; else &#123; mEngine.start(); &#125; &#125; 以上，就是本次项目的主要思路了。相关注释代码里都写的很清楚了，项目地址在这里。仿薄荷Loading动画，大家走过路过千万别忘了给个Star啊。","path":"2017/08/19/bohe_path/","date":"08-19","excerpt":"","tags":[{"name":"Canvas动画","slug":"Canvas动画","permalink":"https://yoursite.com/tags/Canvas动画/"}],"preview":"https://raw.githubusercontent.com/JadynAi/oldpage.io/master/img/20170819-blog-header-bg.jpg"},{"title":"实用性MAX!像普通View一样自由使用的粒子组件","text":"原创文章，转载请注明出处。之前作过一篇萤火虫飞舞粒子效果，当时看还不错。无论是性能还是UI都满足了当时的设计效果，但实际应用到项目中，却发现由于SurfaceView其本身是绘制在window层面上的，对View本身的属性有很多的限制，用起来却不是很实在，还存在着很多不足。 于是便将之前的效果重新写了一下，改用继承View来实现，虽然说和SurfaceView相比，在绘制性能上有那么一丝丝的不足 。但轮子本质的含义还是为了服务于项目，项目中方便的使用才是最重要的。 阅读本文，大概需要三分半钟。如果需要直观看代码的话请点这里点这里！！首先看一下效果图 接着分析实现过程中的几个问题 如何保持不间断的绘制 粒子的运动轨迹控制（随机方向，碰到边界回弹以及旋转） 问题1：如何保持不间断的绘制​ View 的粒子绘制本身实在onDraw中进行的，所以最开始我的方案是在canvas绘制完一波之后，继续调用 invalidate()方法，这样就形成了一个死循环，就达到了不间断重复绘制的效果。 1234567891011@Overrideprotected void onDraw(Canvas canvas) &#123; super.onDraw(canvas); canvas.save(); //粒子的一波绘制 for (Particle circle : mCircles) &#123; circle.drawItem(canvas); &#125; canvas.restore(); invalidate();&#125; 这里需要注意的是，canvas的绘制是一个阻塞的过程，也就是从canvas.save()方法之后，一直到invalidate()之前，是阻塞的。 重绘是会一直等到所有的粒子绘制完成之后，才会继续调起的。 Tips： canvas的save和restore方法是搭配使用的。save存储之前的canvas状态，restore恢复save之前的状态。 save方法是可以多次使用的，可以搭配使用的是方法restoreToCount(saveCount)。参数saveCount从1开始计数，表示可以恢复到第几次save之前的状态。 ​ 这种方式的缺点在demo完成之后很明显的体现了出来。第一，速度不可控制，譬如有些时候恰恰需要粒子变慢一点呢。使用这种方式就不太好实现了。第二，粒子动画的播放和暂停实现起来不优雅，诚然写一个布尔值来控制也可以，但也难免………………太不优雅了吧。反正我个人是比较不喜欢写这种代码的。 ​ 那么，最终我的实现方式，是采用了属性动画来实现的，没错——就是ValueAnimator。来看代码： 123456789101112private ValueAnimator mParticleAnim;-----------------mParticleAnim = ValueAnimator.ofInt(0).setDuration(30);mParticleAnim.setRepeatCount(ValueAnimator.INFINITE);mParticleAnim.addListener(new AnimatorListenerAdapter() &#123; @Override public void onAnimationRepeat(Animator animation) &#123; super.onAnimationRepeat(animation); Log.d(TAG, &quot;onAnimationRepeat: &quot; + System.currentTimeMillis()); invalidate(); &#125;&#125;); 这段代码展示出来，你懂得。 在一个无限循环的属性动画里，在Repeat监听事件里，调用View的invalidate方法重绘。这样每次触发重绘的时间间隔就是属性动画的持续时间。 要是想控制粒子的运动速率，那么只需要调整动画的持续时间即可。 而且如果要对外暴露粒子动画开始或者停止的方法，只需要控制属性动画的start和stop就行了。 问题2：粒子的运动轨迹​ 在构建轮子的时候，思路其实一直都很清晰。View层级主要是调起和控制绘制。具体要绘制什么东西和路线的控制都由粒子对象内部来实现。这样就可以贯彻单一原则，各自负责各自的东西，降低耦合性。 ​ 我们来看一下粒子对象Particle内的代码： 1234567891011121314151617181920212223242526272829303132333435public Particle(Bitmap drawBitmap, Matrix matrix, Paint paint, float x, float y, int width, int height) &#123; //绘制的bitmap对象和矩阵对象，矩阵用来控制旋转和运动方向 mDrawBitmap = drawBitmap; mBitmapMatrix = matrix; mDrawBitmapWidth = drawBitmap.getWidth(); mDrawBitmapHeight = drawBitmap.getHeight(); mBitmapCenterX = mDrawBitmapWidth / 2f; mBitmapCenterY = mDrawBitmapHeight / 2f; //画笔对象 mPaint = paint; //view 的宽和高用来判断边界 this.mWidth = width; this.mHeight = height; //粒子运动的坐标 this.mX = x; this.mY = y; //粒子的开始坐标 mStartX = x; mStartY = y; //x 和y轴的运动方向选择，随机函数 mIsAddX = mRandom.nextBoolean(); mIsAddY = mRandom.nextBoolean(); setRandomParm();&#125;private void setRandomParm() &#123; //x 和 y轴每次运动的距离和每次旋转的角度，随机值 mDisX = mRandom.nextInt(2) + 1.2f; mDisY = mRandom.nextInt(2) + 1.2f; mAddDegree = mRandom.nextInt(5) + 3f;&#125; ​ 运动轨迹这方面只需要随机出来x和y轴的方向，还有每次递增或者递减的值即可。怎么样是不是丝毫没有技术难度啊。好了，看绘制和到边界的处理代码吧。 123456789101112131415161718192021222324252627282930313233343536public void drawItem(Canvas canvas) &#123; //绘制 mBitmapMatrix.reset(); mBitmapMatrix.preTranslate(mX += getPNValue(mIsAddX, mDisX), mY += getPNValue(mIsAddY, mDisY)); mBitmapMatrix.preRotate(mDegrees += mAddDegree, mBitmapCenterX, mBitmapCenterY); canvas.drawBitmap(mDrawBitmap, mBitmapMatrix, mPaint); Log.d(TAG, &quot;mX : &quot; + mX); Log.d(TAG, &quot;mY : &quot; + mY); judgeOutline();&#125;private void judgeOutline() &#123; boolean judgeX = mX &lt;= 0 || mX &gt;= (mWidth - mDrawBitmapWidth); boolean judgeY = mY &lt;= 0 || mY &gt;= (mHeight - mDrawBitmapHeight); if (judgeX) &#123; mIsAddX = !mIsAddX; mIsAddY = mRandom.nextBoolean(); setRandomParm(); if (mX &lt;= 0) &#123; mX = 0; &#125; else &#123; mX = mWidth - mDrawBitmapWidth; &#125; return; &#125; if (judgeY) &#123; mIsAddY = !mIsAddY; mIsAddX = mRandom.nextBoolean(); setRandomParm(); if (mY &lt;= 0) &#123; mY = 0; &#125; else &#123; mY = mHeight - mDrawBitmapHeight; &#125; &#125;&#125; ​ 以上就是粒子对象内部的运动轨迹和边界判断代码了，怎样，是不是超级简单呢？ ​ 好了，如果你喜欢我的文章的话，那么请不要犹豫，给我一个star吧。GitHub地址：这里这里！！","path":"2017/06/28/android_partical/","date":"06-28","excerpt":"","tags":[{"name":"Canvas动画","slug":"Canvas动画","permalink":"https://yoursite.com/tags/Canvas动画/"}],"preview":"https://raw.githubusercontent.com/JadynAi/oldpage.io/master/img/20170628-blog-header-bg.jpg"},{"title":"低消耗、带回滚动画的仿探探交互效果","text":"原创文章，转载请注明出处。 前段时间项目中有一个仿探探的交互界面，写代码的过程中，觉得效果还不错。就把思路和原理记录了下来，分享一下。 先奉上GitHub地址，大家感兴趣可以来个starInfinateCard 套路还是要走一下的，先给大家看一下效果图: 真机上一点都不卡！一点都不卡！不卡！！！ 又学习了一些录制GIF的方法，这次来一张真机的效果！！ 会用到的知识点 Recyclerview的layoutManager实现布局 使用ItemTouchHelper处理滑动事件 手指滑动过程中，view的UI渐变（透明度或者其它） RecyclerView的ItemAnimator实现回滚动画 1、LayoutManger众所周知，Recyclerview之所以强大，完全在于它百变的适应性。它能实现任何你想要的布局样式，而它的奥秘就在于LayoutManger。 ​ 本次项目中我们UI效果仿照探探的样式，是卡牌样式的堆叠效果，按照List集合的顺序沿着Z轴纵向深处排列。这里有一个值得注意的Tip，子view是按照list集合的顺序去绘制的，也就是在这个我们自定义的LayoutManger里，第二个view会覆盖第一个，第三个会覆盖第二个，以此类推。如果我们想要第一眼就看到List集合的第一个，那么必须将list集合reverse后，再来绘制。 ​ 好了，我们来看代码。LayoutManger的精髓其实就在于onLayoutChildren（……）这个方法，通过这个方法来实现自定义的布局。 12345678910111213141516171819@Overridepublic void onLayoutChildren(RecyclerView.Recycler recycler, RecyclerView.State state) &#123; int itemCount = getItemCount(); // 代码的稳健之道，就在于该保护的地方一定要保护 if (itemCount == 0) &#123; return; &#125; detachAndScrapAttachedViews(recycler); for (int position = 0; position &lt; itemCount; position++) &#123; View view = recycler.getViewForPosition(position); addView(view); measureChildWithMargins(view, 0, 0); int widthSpace = getWidth() - getDecoratedMeasuredWidth(view); int heightSpace = getHeight() - getDecoratedMeasuredHeight(view); // recyclerview 布局 layoutDecoratedWithMargins(view, widthSpace / 2, heightSpace / 2, widthSpace / 2 + getDecoratedMeasuredWidth(view), heightSpace / 2 + getDecoratedMeasuredHeight(view)); &#125; ​ 很多人看到这里，肯定会喷出一句“卧槽！这就完了，就这点代码？” ​ 没错，如果不追求精细的话，这点代码确实可以搞定这个布局样式。现在我们来逐行分析一下代码。 detachAndScrapAttachedViews(recycler)这个方法就是将所有的view缓存在scrap里。Recyclerview有二级缓存，scrap和Recycle。使用Detach方式处理的view缓存在scrap里，用的时候不需要重新绑定数据。Remove方式处理的view缓存在Recycle里，使用的时候会重新绑定数据。 接下来的for循环代码就简单的多了，无非就是获得view的宽高信息，将其布局在Recyclerview内 当然，如果只是以上那些简单的代码，未免也太对不起Recyclerview了。毕竟Recyclerview最强大的地方就是对view的回收和利用了，要不然为什么叫Recycler呢。 对子view的回收利用​ 首先这种卡牌叠层的交互模式，不需要展示那么多的view，也就是我们仅仅需要让前几个view展示出来就可以了。其他的view，放在scrap缓存里即可。 12345678910111213141516171819202122232425262728293031@Overridepublic void onLayoutChildren(RecyclerView.Recycler recycler, RecyclerView.State state) &#123; int itemCount = getItemCount(); // 代码的稳健之道，就在于该保护的地方一定要保护 if (itemCount == 0) &#123; return; &#125; detachAndScrapAttachedViews(recycler); // 测量子view的位置信息并储存 for (int position = 0; position &lt; itemCount; position++) &#123; // 根据position获取一个碎片view，可以从回收的view中获取，也可能新构造一个 View view = recycler.getViewForPosition(position); Log.d(TAG, &quot;recycler&quot; + view.getTag().toString()); addView(view); if (mViewInfo == null) &#123; // 计算此碎片view包含边距的尺寸 measureChildWithMargins(view, 0, 0); // getDecoratedMeasuredWidth方法是获取此碎片view包含边距和装饰的宽度width int widthSpace = getWidth() - getDecoratedMeasuredWidth(view); int heightSpace = getHeight() - getDecoratedMeasuredHeight(view); mViewInfo = new Rect(); int left = widthSpace / 2; int top = heightSpace / 2; int right = widthSpace / 2 + getDecoratedMeasuredWidth(view); int bottom = heightSpace / 2 + getDecoratedMeasuredHeight(view); mViewInfo.set(left, top, right, bottom); &#125; detachAndScrapView(view, recycler); &#125; LayoutItems(recycler, state);&#125; 我们将view的位置信息使用一个Rect对象来保存，因为layoutDecorated(View child, int left, int top, int right, int bottom)这个函数的参数是整型。Rect和RecF两个对象最大的区别就是精度区别了。 然后将每个子view通过方法detachAndScrapView缓存到scrap内 最后通过LayoutItems(recycler, state)方法将需要展示的view展示出来，注释已经很清楚了哈 1234567891011121314151617181920212223/** * 回收不需要的Item，并且将需要显示的Item从缓存中取出 */private void LayoutItems(RecyclerView.Recycler recycler, RecyclerView.State state) &#123; // 当数量大于临界点才需要回收view boolean isMeetNum = getItemCount() &gt; CardConfig.MAX_SHOW_INDEX + 1; if (isMeetNum) &#123; for (int i = CardConfig.MAX_SHOW_INDEX + 1; i &lt; getItemCount(); i++) &#123; View child = recycler.getViewForPosition(i); removeAndRecycleView(child, recycler); &#125; &#125; // 展示需要展示的view for (int i = isMeetNum ? CardConfig.MAX_SHOW_INDEX : getItemCount() - 1; i &gt;= 0; i--) &#123; View scrap = recycler.getViewForPosition(i); measureChildWithMargins(scrap, 0, 0); addView(scrap); //将这个item布局出来 layoutDecorated(scrap, mViewInfo.left, mViewInfo.top, mViewInfo.right, mViewInfo.bottom); int translateY = i * CardConfig.CARD_VERTICAL_GAP; ViewCompat.setTranslationY(scrap, -translateY); &#125;&#125; 这个方法中，将不需要展示的view全部remove，然后将需要展示的view布局出来。 2、使用ItemTouchHelper实现滑动​ ItemTouchHelper是一个为Recyclerview提供 Swipe、drag、drop事件的工具类。使用方法也很简单，推荐大家看泡网的这一片入门文章，思路很清晰。 ​ 本次项目中使用的是ItemTouchHelper本身提供的一个帮助类 SimpleCallback，使用方法其实很简单。它的构造参数有两个值，一个是dragDirs长按的方向，另一个是swipeDirs滑动的方向。可以看看SImpleCallBack的源码： 1234public SimpleCallback(int dragDirs, int swipeDirs) &#123; mDefaultSwipeDirs = swipeDirs; mDefaultDragDirs = dragDirs;&#125; ​ 而我们只需要滑动，所以构造参数中只需要实现 swipe 即可： 12//不支持长按拖拽，支持swipe，而且四个方向皆可以swipethis(0, ItemTouchHelper.UP | ItemTouchHelper.DOWN | ItemTouchHelper.LEFT | ItemTouchHelper.RIGHT） ​ 然后应用此 ItemTouchHelper 即可： 123CardItemTouchHelperCallback cardCallback = new CardItemTouchHelperCallback(mRecyclerView, mRecyclerView.getAdapter(), list);ItemTouchHelper touchHelper = new ItemTouchHelper(cardCallback);touchHelper.attachToRecyclerView(mRecyclerView); ​ 到这里 Recyclerview已经实现了四个方向的滑动了，但滑动之后的操作还需要再实现一下。我们在 onSwiped(RecyclerView.ViewHolder viewHolder, int direction) 方法中实现 swipe之后的操作。这个方法有两个参数，viewHolder代表的就是此时滑动的viewholder，direction 代表的是这个view最终滑动的方向。在这个方法里，我们对数据源进行操作，然后刷新列表。 在Recyclerview的Adapter的数据刷新上，我使用了扩展包提供的 DiffUtils,是google提供的替换 notifyDataSetChanged()无脑刷 的方案。 ​ 最后在 onChildDraw(Canvas c, RecyclerView recyclerView, RecyclerView.ViewHolder viewHolder, float dX, float dY, int actionState, boolean isCurrentlyActive) 方法中实现滑动时的动画。这个方法有多达七个参数，我来依次解释一下： c ，Recyclerview用来绘制children的画笔 recyclerView，额……就是所依赖的Recyclerview viewHolder，当下滑动的这个view的viewHolder dX、dY，手指在控制滑动的时候，此view水平X轴和垂直Y轴位移的距离，单位像素 actionState，标明此时是长按拖拽还是单纯的swipe isCurrentlyActive，标明此时滑动的view是处于手指控制状态，还是手指松开后的回弹动画状态 ​ 了解了参数之后，在这个方法中就可以实现滑动时的动画了。SimpleCallBack 默认的对滑动距离判断的条件是，水平方向是Recyclerview宽的一半，垂直方向是Recyclerview高的一半。 1234//这个方法返回的值就是默认的阙值，想要更灵敏的话只需在自定义CallBack中重写这个方法，将值变小。更迟钝的话则反之public float getSwipeThreshold(ViewHolder viewHolder) &#123; return .5f;&#125; ​ 撸出来的代码如下： 1234567891011121314151617181920212223242526272829303132@Override public void onChildDraw(Canvas c, RecyclerView recyclerView, RecyclerView.ViewHolder viewHolder, float dX, float dY, int actionState, boolean isCurrentlyActive) &#123; super.onChildDraw(c, recyclerView, viewHolder, dX, dY, actionState, isCurrentlyActive);// Log.d(TAG, &quot;onChildDraw dX : &quot; + dX);// Log.d(TAG, &quot;onChildDraw dY : &quot; + dY); Log.d(TAG, &quot;onChildDraw isCurrentlyActive: &quot; + isCurrentlyActive); if (mItemW == 0) &#123;// Log.d(TAG, &quot;onChildDraw getAdapterPosition : &quot; + viewHolder.getAdapterPosition()); mItemW = viewHolder.itemView.getWidth(); mHorJudgeDistance = recyclerView.getWidth() * getSwipeThreshold(viewHolder); mVerJudgeDistance = recyclerView.getHeight() * getSwipeThreshold(viewHolder); &#125; float ratio; if (Math.abs(dX) &gt; Math.abs(dY)) &#123; //以宽为判定基准 ratio = Math.abs(dX) / mHorJudgeDistance; &#125; else &#123; //以高为判定基准 ratio = Math.abs(dY) / mVerJudgeDistance; &#125; float realRatio = ratio &gt;= 1f ? 1f : ratio; ViewCompat.setAlpha(viewHolder.itemView, 1 - realRatio); boolean isMeetNum = recyclerView.getLayoutManager().getItemCount() &gt; CardConfig.MAX_SHOW_COUNT + 1; int maxJudge = isMeetNum ? CardConfig.MAX_SHOW_COUNT - 1 : (recyclerView.getLayoutManager().getItemCount() - 1); for (int i = 1; i &lt;= maxJudge; i++) &#123; View itemView = recyclerView.findViewHolderForAdapterPosition(i).itemView; float v = i * CardConfig.CARD_VERTICAL_GAP - realRatio * CardConfig.CARD_VERTICAL_GAP; ViewCompat.setTranslationY(itemView, -v); &#125; &#125; 3、使用ItenAnimator实现回滚动画​ ItemAnimator 我没有选择实现，而是使用了现成的轮子recyclerview-animators，没有选择远程库引入。而是将源代码copy进来，再进行了适当性的修改。 12mRecyclerView.setItemAnimator(new SlideAnimator());mRecyclerView.getItemAnimator().setAddDuration(250); ​ OK，到这里基本上就大功告成了，只剩下一些小细节和bug处理一下即可。在CallBack 类的 onSwipe 方法中将 direction 赋值到一个静态变量中，然后在 SlideAnimator 根据不同的方向实现不同的动画。 ​ 在实现过程中，我发现回滚动画的那个view居然不是滑走的view，就使用了一个单例来管理数据。动画开始时将数据设置为滑走的view的数据，动画结束后再将动画view的数据还原。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Overrideprotected void preAnimateAddImpl(RecyclerView.ViewHolder holder) &#123; Log.d(TAG, &quot;preAnimateAddImpl: &quot; + holder.itemView.getTag().toString()); DataExchangeMgr.getInstance().saveOrignData((int) holder.itemView.getTag(R.id.view_data)); //根据不同的方向设置不同的初始值 if (isHorizDirection()) &#123; int width = holder.itemView.getRootView().getWidth(); ViewCompat.setTranslationX(holder.itemView, CardConfig.sViewholderDirection == ItemTouchHelper.LEFT ? -width : width); &#125; else &#123; int height = holder.itemView.getRootView().getHeight(); ViewCompat.setTranslationY(holder.itemView, CardConfig.sViewholderDirection == ItemTouchHelper.UP ? -height : height); &#125;&#125;@Overrideprotected void animateAddImpl(final RecyclerView.ViewHolder holder) &#123; //根据不同的方向选择不同的动画 ViewPropertyAnimatorCompat viewPropertyAnimatorCompat = ViewCompat.animate(holder.itemView) .setDuration(getAddDuration()) .setInterpolator(mInterpolator) .setListener(new DefaultAddVpaListener(holder) &#123; @Override public void onAnimationStart(View view) &#123; super.onAnimationStart(view); //将此view数据设置为滑开的数据 ((ImageView) mViewHolder.itemView.findViewById(R.id.show_img)).setImageResource(DataExchangeMgr.getInstance().getCurrentData()); &#125; @Override public void onAnimationCancel(View view) &#123; super.onAnimationCancel(view); //数据还原 ((ImageView) mViewHolder.itemView.findViewById(R.id.show_img)).setImageResource(DataExchangeMgr.getInstance().getOrignalData()); &#125; @Override public void onAnimationEnd(View view) &#123; super.onAnimationEnd(view); //数据还原 ((ImageView) mViewHolder.itemView.findViewById(R.id.show_img)).setImageResource(DataExchangeMgr.getInstance().getOrignalData()); &#125; &#125;) .setStartDelay(50); if (isHorizDirection()) &#123; viewPropertyAnimatorCompat .translationX(0) .start(); &#125; else &#123; viewPropertyAnimatorCompat .translationY(0) .start(); &#125;&#125;private boolean isHorizDirection() &#123; return CardConfig.sViewholderDirection == ItemTouchHelper.LEFT || CardConfig.sViewholderDirection == ItemTouchHelper.RIGHT;&#125; 最后感兴趣的可以看看代码，奉上GitHub地址。InfinateCard","path":"2017/05/10/android-infinite-card/","date":"05-10","excerpt":"","tags":[{"name":"交互体验","slug":"交互体验","permalink":"https://yoursite.com/tags/交互体验/"}],"preview":"https://raw.githubusercontent.com/JadynAi/oldpage.io/master/img/20170510-blog-header-bg.jpg"},{"title":"Android萤火虫飞舞粒子效果","text":"GitHub地址 原创文章，转载请注明出处 萤火虫飞舞粒子效果 本项目中我提供了两种方案，最终呈现的效果如下： 先奉上GitHub地址戳这里，有兴趣的同鞋star一下咯 实现原理Android的粒子效果、粒子动画，已经有很多开源的轮子了。作为一个坚定的轮子主义者，我google了大半天，却没有找到这种类似于萤火虫飞舞的效果。只好自己来实现这种效果。 相比较普通的View，SurfaceView更加适合这种不断变化的画面，所以选择SurfaceView来实现。现在把思路再重新梳理一下： 大小不同的粒子在区域内随机分布 粒子做无规则运动，然后消失 粒子区域内随机分布这个简单，我们在callBack的方法内直接循环生成一个粒子的数组即可。方位的话使用Random即可。 1234567891011121314151617if (mCircles.size() == 0) &#123; for (int i = 0; i &lt; MAX_NUM; i++) &#123; FloatParticleLine f = new FloatParticleLine(getF() * mMeasuredWidth, getF() * mMeasuredHeight, mMeasuredWidth, mMeasuredHeight); f.setRadius(mRandom.nextInt(2) + 1.2f); mCircles.add(f); &#125;&#125;private float getF() &#123; float v = mRandom.nextFloat(); if (v &lt; 0.2f) &#123; return v + 0.2f; &#125; else if (v &gt;= 0.85f) &#123; return v - 0.2f; &#125; else &#123; return v; &#125; &#125; getF（）方法是限制在区域内取值，mMeasuredWidth、mMeasuredHeight为SurfaceView的宽和高。 这里的宽和高在粒子对象FloatParticleLine，内会用到。 然后我们在创建一个线程，在run（）方法内做无线循环的绘制即可，为了避免无意义的绘制，可以使用Thread.sleep方法来控制帧数。 1234567891011121314151617181920212223while (isRun) &#123; try &#123; mCanvas = mHolder.lockCanvas(null); if (mCanvas != null) &#123; synchronized (mHolder) &#123; // 清屏 mCanvas.drawColor(Color.TRANSPARENT, PorterDuff.Mode.CLEAR); for (FloatParticleLine circle : mCircles) &#123; circle.drawItem(mCanvas); &#125; // 控制帧数 Thread.sleep(25); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if (mCanvas != null) &#123; mHolder.unlockCanvasAndPost(mCanvas); &#125; &#125;&#125; isRun的变量我们会在SurfaceView内callBack的surfaceDestroyed方法中置为false 粒子做无规则运动 方案一 其实看到这种粒子效果，首先应该想到的就是Canvas了。 在SurfaceView里就是通过不断地循环调用FloatParticleLine类的drawItem（）方法来实现粒子的运动。我第一种方案的实现，就是每一个粒子在被创建出来的时候，就随机选择一个方向开始运动，滑过一定的轨迹之后让其消失就好了。 至于怎么选择随机方向，我这里的做法是，分别随机生成一个x和y轴上的递增或者递减的数值，然后每次在前一次绘制的基础上，x和y分别递增递减，直到运动到屏幕边缘或者是规定的运动距离满足了再消失即可。 12345678910111213//随机生成参数private void setRandomParm() &#123; // 2017/5/2-上午10:47 x和y的方向 mIsAddX = mRandom.nextBoolean(); mIsAddY = mRandom.nextBoolean(); // 2017/5/2-上午10:47 x和y的取值 mDisX = mRandom.nextInt(2) + 0.2f; mDisY = mRandom.nextInt(2) + 0.3f; // 2017/5/2-上午10:47 内部区域的运动最远距离 mDistance = mRandom.nextInt((int) (0.25f * mWidth)) + (0.125f * mWidth);&#125; 绘制图形： 1234567891011121314151617181920212223242526272829303132public void drawItem(Canvas canvas) &#123; if (mX == mStartX) &#123; mPaint.setAlpha(ALPHA_MAX); &#125; //绘制 canvas.drawCircle(mX += getPNValue(mIsAddX, mDisX), mY += getPNValue(mIsAddY, mDisY), mRadius, mPaint); //内部区域运动到一定距离消失 if (judgeInner()) &#123; float gapX = Math.abs(mX - mStartX); float ratio = 1 - (gapX / mDistance); mPaint.setAlpha((int) (255 * ratio)); mRadius = mStartRadius * ratio; if (gapX &gt;= mDistance || mY - mStartY &gt;= mDistance) &#123; resetDisXY(); return; &#125; return; &#125; //外部区域运动到屏幕边缘消失 if (judgeOutline()) &#123; resetDisXY(); &#125;&#125;private void resetDisXY() &#123; setRandomParm(); mPaint.setAlpha(0); mX = mStartX; mY = mStartY; mRadius = mStartRadius; &#125; judgeInner()和judgeOutline()是判断区域的方法，内部区域的点和外部区域的店消失时机不同 在透明度为0也就是粒子消失时，让粒子回到原点，再重新选择一个方向，进行下一步运动轨迹。 方案二 方案二粒子做的运动是贝塞尔曲线，函数实在网上找到的一个函数。每当粒子做完一次曲线运动后，再随机生成一段新的贝塞尔曲线即可。 思路和方案一的思路都是一样的，无非就是运动的轨迹不同而已。 总结做完之后回头再看，发现这个项目的原理其实并不难，可以说是简单了。但刚开始起步的时候真的还是比较懵的，原因就是没有思路。 所以做任何效果，思路最重要。","path":"2017/04/28/android-firefly/","date":"04-28","excerpt":"","tags":[{"name":"Canvas动画","slug":"Canvas动画","permalink":"https://yoursite.com/tags/Canvas动画/"}],"preview":"https://raw.githubusercontent.com/JadynAi/oldpage.io/master/img/20170428-blog-header-bg.jpg"},{"title":"Hello World","text":"Hello World","path":"2017/04/28/hello-world/","date":"04-28","excerpt":"","tags":[]}]}